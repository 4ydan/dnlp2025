{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37853986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import preprocess\n",
    "importlib.reload(preprocess)\n",
    "from preprocess import download_squad_dataset, process_split, write_to_files\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904dece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "eval_dataset = process_split(eval, nlp)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f14537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = process_split(train)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")\n",
    "#t_context_tokens, t_question_tokens, t_answer_tokens, t_span_tokens = write_to_files(train_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d61b",
   "metadata": {},
   "source": [
    "## Map tokens to embedding indices\n",
    "\n",
    "- load GloVe embeddings\n",
    "- map vocabulary to embedding indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luca/workspace/dnlp2025/glove_embeddings/glove.840B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "glove_path = os.path.abspath(os.path.dirname(os.getcwd())) + \"/glove_embeddings/glove.840B.300d.txt\"\n",
    "print(glove_path) \n",
    "assert os.path.exists(glove_path), (\"glove embeddings file missing!\")\n",
    "embedding_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        vals = line.split(' ')\n",
    "        word = vals[0]\n",
    "        coefs = np.asarray(vals[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Done! \", len(embedding_index),\"words loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61b13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m idx2word\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m idx2word\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m embedding_matrix\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros(embedding_dim, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m embedding_matrix\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mzeros(embedding_dim, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_or_create_index\u001b[39m(token):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_dim = 300\n",
    "embedding_matrix = []\n",
    "\n",
    "word2idx[\"[PAD]\"] = 0\n",
    "word2idx[\"[UNK]\"] = 1\n",
    "idx2word.append(\"[PAD]\")\n",
    "idx2word.append(\"[UNK]\")\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "\n",
    "def get_or_create_index(token):\n",
    "    token_lower = token.lower()\n",
    "    if(token_lower) in word2idx:\n",
    "        return word2idx[token_lower]\n",
    "    else:\n",
    "        idx = len(word2idx)\n",
    "        word2idx[token_lower] = idx\n",
    "        idx2word.append(token_lower)\n",
    "        if token_lower in embedding_index:\n",
    "            embedding_matrix.append(embedding_index[token_lower])\n",
    "        else:\n",
    "            embedding_matrix.append(np.random.normal(scale=0.01, size=embedding_dim))\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0302f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n",
      "31338\n",
      "25764\n",
      "25764\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 12, 15, 8, 16, 17, 18, 19, 20, 12, 21, 22, 23, 12, 7, 8, 24, 17, 25, 19, 13, 26, 27, 28, 12, 15, 8, 24, 17, 29, 19, 13, 30, 31, 32, 33, 34, 10, 35, 36, 37, 2, 3, 38, 23, 12, 9, 5, 39, 40, 41, 42, 43, 44, 43, 45, 46, 47, 48, 12, 49, 50, 51, 52, 45, 53, 54, 43, 55, 23, 56, 57, 5, 12, 58, 2, 3, 43, 12, 16, 59, 12, 60, 61, 62, 60, 63, 64, 65, 66, 67, 68, 43, 56, 69, 56, 70, 71, 12, 72, 14, 73, 74, 2, 3, 9, 63, 75, 76, 17, 77, 78, 12, 9, 79, 80, 81, 82, 56, 60, 2, 3, 83, 60, 19, 43, 84, 85, 12, 86, 87, 88, 89, 12, 90, 76, 4, 23]\n"
     ]
    }
   ],
   "source": [
    "sen_idxs = []\n",
    "#do this for every token in contexts,question and answers\n",
    "all_tokens = []\n",
    "all_tokens.extend(context_tokens)\n",
    "all_tokens.extend(question_tokens)\n",
    "all_tokens.extend(answer_tokens)\n",
    "print(len(all_tokens))\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    if tokens is None:\n",
    "        continue\n",
    "    idx = [get_or_create_index(t) for t in tokens.split()]\n",
    "    sen_idxs.append(idx)\n",
    "print(len(sen_idxs))\n",
    "print(len(word2idx))\n",
    "print(len(idx2word))\n",
    "print(sen_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c9ee549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25764, 300)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f147bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True, dropout=dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.w = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.sentinel = nn.Parameter(torch.randn(1,hidden_dim))\n",
    "\n",
    "    def encode_sequence(self, idxs, mask):\n",
    "        lengths = mask.sum(dim=1)  # [batch]\n",
    "        sorted_lens, sorted_idx = lengths.sort(descending=True)\n",
    "        _, orig_idx = sorted_idx.sort()\n",
    "\n",
    "        # Sort sequences for packing\n",
    "        idxs_sorted = idxs[sorted_idx]\n",
    "        emb = self.embedding(idxs_sorted)\n",
    "        packed = pack_padded_sequence(emb, sorted_lens.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # LSTM encoding\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [batch, max_len, hidden]\n",
    "        out = self.dropout(out)\n",
    "        out = out[orig_idx]  # restore original order\n",
    "\n",
    "        # Insert sentinel at end-of-sequence index for each example\n",
    "        batch_size = out.size(0)\n",
    "        sentinel_expanded = self.sentinel.expand(batch_size, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "\n",
    "        out_with_sentinel = torch.cat([out, torch.zeros_like(sentinel_expanded)], dim=1)  # [batch, max_len+1, hidden]\n",
    "        lens = lengths.long().unsqueeze(1).unsqueeze(2).expand(-1, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "        out_with_sentinel = out_with_sentinel.scatter(1, lens, sentinel_expanded)\n",
    "\n",
    "        return out_with_sentinel  # [batch, seq_len + 1, hidden]\n",
    "\n",
    "    def forward(self, doc_idxs, doc_mask, q_idxs, q_mask):\n",
    "        \"\"\"\n",
    "        doc_idxs/q_idxs: [batch, seq_len]\n",
    "        doc_mask/q_mask: [batch, seq_len]\n",
    "        \"\"\"\n",
    "        D = self.encode_sequence(doc_idxs, doc_mask)  # [batch, m+1, hidden]\n",
    "        Q_prime = self.encode_sequence(q_idxs, q_mask)  # [batch, n+1, hidden]\n",
    "\n",
    "        # Nonlinear projection: Q = tanh(W * Q′ + b)\n",
    "        Q = torch.tanh(self.w(Q_prime) + self.b)  # [batch, n+1, hidden]\n",
    "\n",
    "        return D, Q       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9b422",
   "metadata": {},
   "source": [
    "## Small Encoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document encoding shape (with sentinel): torch.Size([1, 6, 64])\n",
      "Question encoding shape (projected): torch.Size([1, 3, 64])\n",
      "\n",
      "Sentinel vector (document): tensor([ 1.7383,  0.8497, -1.3716, -1.5591,  0.0837,  0.1320,  1.2961, -1.5653,\n",
      "         0.3502,  0.5081,  0.9126, -0.6087,  0.0031,  0.3404, -0.7516, -0.1366,\n",
      "         2.1637, -0.0730,  1.2716,  0.6490, -0.7126, -0.4458,  0.4099, -0.4572,\n",
      "        -0.2072, -0.8879,  0.2099,  0.3814,  2.4512,  0.1368, -1.0433, -0.3454,\n",
      "        -1.0600,  1.3232,  0.1469,  0.8049,  0.4266, -0.1950, -1.8880, -1.1639,\n",
      "         1.2693,  0.3779,  0.1997,  1.1024,  0.9713, -0.2005, -0.3045,  0.4434,\n",
      "         0.3661,  1.2391,  0.9023,  1.7191, -0.6880, -0.4479,  1.0720,  1.0619,\n",
      "        -1.1908, -0.0930, -0.0781, -0.3598, -0.9050, -0.7227,  1.1644,  0.5092],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Sentinel vector (question): tensor([ 0.5440, -0.0703, -0.1595, -0.1310,  0.8475, -0.5078,  0.2081,  0.8371,\n",
      "         0.5305, -0.4063,  0.0593,  0.1951, -0.6979,  0.7548,  0.1480,  0.1267,\n",
      "        -0.7959,  0.4877,  0.1987,  0.1724, -0.0420,  0.7170, -0.0023,  0.0384,\n",
      "        -0.5835, -0.5816, -0.2392, -0.0379, -0.0554,  0.3840, -0.8773, -0.6905,\n",
      "        -0.3169, -0.4159,  0.1525, -0.8344, -0.8218,  0.5057,  0.4873, -0.6062,\n",
      "        -0.1058,  0.2836, -0.0452,  0.1128, -0.2085, -0.2302,  0.4721, -0.0604,\n",
      "        -0.3558,  0.0274,  0.3254, -0.0132,  0.1784,  0.3327,  0.9704,  0.1429,\n",
      "        -0.6974, -0.4099, -0.1405,  0.0139,  0.4372,  0.5586, -0.8282, -0.2416],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Dummy vocab\n",
    "word2idx = {\n",
    "    \"[PAD]\": 0, \"[UNK]\": 1,\n",
    "    \"the\": 2, \"quick\": 3, \"brown\": 4, \"fox\": 5, \"jumps\": 6, \"over\": 7, \"lazy\": 8, \"dog\": 9\n",
    "}\n",
    "\n",
    "# Random embedding matrix for vocab (vocab_size x emb_dim)\n",
    "vocab_size = len(word2idx)\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Dummy inputs\n",
    "# Document: \"the quick brown fox jumps\"\n",
    "# Question: \"the fox\"\n",
    "doc_tokens = [2, 3, 4, 5, 6]\n",
    "q_tokens = [2, 5]\n",
    "\n",
    "# Padding to max length\n",
    "doc_max_len = 6\n",
    "q_max_len = 4\n",
    "doc_input = [doc_tokens + [0] * (doc_max_len - len(doc_tokens))]  # batch size 1\n",
    "q_input = [q_tokens + [0] * (q_max_len - len(q_tokens))]\n",
    "\n",
    "# Masks (1 for real tokens, 0 for padding)\n",
    "doc_mask = [[1]*len(doc_tokens) + [0]*(doc_max_len - len(doc_tokens))]\n",
    "q_mask = [[1]*len(q_tokens) + [0]*(q_max_len - len(q_tokens))]\n",
    "\n",
    "# Convert to tensors\n",
    "doc_idxs = torch.tensor(doc_input)      # [1, 6]\n",
    "doc_mask = torch.tensor(doc_mask)       # [1, 6]\n",
    "q_idxs = torch.tensor(q_input)          # [1, 4]\n",
    "q_mask = torch.tensor(q_mask)           # [1, 4]\n",
    "\n",
    "hidden_size = 64\n",
    "encoder = Encoder(hidden_size, embedding_matrix, 0)\n",
    "\n",
    "# Run encoder\n",
    "D, Q = encoder(doc_idxs, doc_mask, q_idxs, q_mask)\n",
    "\n",
    "# Outputs\n",
    "print(\"Document encoding shape:\", D.shape)  # [1, m+1, 64]\n",
    "print(\"Question encoding shape:\", Q.shape)      # [1, n+1, 64]\n",
    "\n",
    "print(\"\\nSentinel vector (document):\", D[0, -1])\n",
    "print(\"Sentinel vector (question):\", Q[0, -1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
