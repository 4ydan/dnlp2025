{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37853986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import download_squad_dataset\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "831521a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:02:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 14.1MB/s]                    \n",
      "2025-06-11 16:02:56 INFO: Downloaded file to /home/luca/stanza_resources/resources.json\n",
      "2025-06-11 16:02:56 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-11 16:02:56 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-06-11 16:02:56 INFO: Using device: cpu\n",
      "2025-06-11 16:02:56 INFO: Loading: tokenize\n",
      "2025-06-11 16:02:56 INFO: Loading: mwt\n",
      "2025-06-11 16:02:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in doc.sentences:\n",
    "        for token in sen.tokens:\n",
    "            tokens.append(token.text)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eecf6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapCharToToken(context, tokens):\n",
    "\n",
    "    concat = \"\"\n",
    "    curr = 0\n",
    "    mapping = {}\n",
    "\n",
    "    for i, char in enumerate(context):\n",
    "        if char != ' ' and char != '\\n':\n",
    "            concat += char\n",
    "            ctoken = tokens[curr]\n",
    "            if concat == ctoken:\n",
    "                start = i - len(concat) + 1\n",
    "                for loc in range(start, i+1):\n",
    "                    mapping[loc] = (concat, curr)\n",
    "                concat = \"\"\n",
    "                curr += 1\n",
    "    if curr != len(tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da811973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008_Summer_Olympics_torch_relay\n",
      "MP3\n",
      "ASCII\n",
      "ASCII\n",
      "ASCII\n",
      "Franco-Prussian_War\n",
      "Franco-Prussian_War\n",
      "Franco-Prussian_War\n",
      "Eritrea\n",
      "Hellenistic_period\n",
      "Hellenistic_period\n",
      "Presbyterianism\n",
      "Presbyterianism\n",
      "Pope_Paul_VI\n",
      "Avicenna\n",
      "Sahara\n",
      "Paris\n",
      "Muammar_Gaddafi\n",
      "The_Bronx\n",
      "mappingissues: 19\n",
      "spanissues: 23\n",
      "tokenissues: 19\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspanissues: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspanissues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenissues: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmappingissues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m     51\u001b[0m context_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m question_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.question\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "i = 0\n",
    "mappingissues = 0\n",
    "spanissues = 0\n",
    "tokenissues = 0\n",
    "dataset = []\n",
    "\n",
    "for article in train[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        context.replace(\"''\",'\" ') \n",
    "        context.replace(\"``\",'\" ') \n",
    "        context_tokens = tokenize(context)\n",
    "        context = context.lower()\n",
    "\n",
    "        mapping = mapCharToToken(context, context_tokens)\n",
    "\n",
    "        if mapping is None:\n",
    "            mappingissues += 1\n",
    "            print(article[\"title\"])\n",
    "            continue\n",
    "        \n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question_tokens = tokenize(qa[\"question\"])\n",
    "\n",
    "            answer_text = qa[\"answers\"][0][\"text\"].lower()\n",
    "            answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            if context[answer_start:answer_end] != answer_text:\n",
    "                spanissues += 1\n",
    "                continue\n",
    "\n",
    "            answer_start_wordloc = mapping[answer_start][1]\n",
    "            answer_end_wordloc = mapping[answer_end-1][1]\n",
    "\n",
    "            answer_tokens = context_tokens[answer_start_wordloc:answer_end_wordloc+1]\n",
    "\n",
    "            if \"\".join(answer_tokens) != \"\".join(answer_text.split()):\n",
    "                tokenissues += 1\n",
    "                continue\n",
    "            dataset.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(answer_tokens), ' '.join([str(answer_start_wordloc), str(answer_end_wordloc)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "94f14537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappingissues: 19\n",
      "spanissues: 23\n",
      "tokenissues: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"mappingissues: {mappingissues}\")\n",
    "print(f\"spanissues: {spanissues}\")\n",
    "print(f\"tokenissues: {mappingissues}\")\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "context_file_path = os.path.join(current_dir, \"data.context\")\n",
    "question_file_path = os.path.join(current_dir, \"data.question\")\n",
    "answer_file_path = os.path.join(current_dir, \"data.answer\")\n",
    "span_file_path = os.path.join(current_dir, \"data.span\")\n",
    "\n",
    "with open(context_file_path,\"w\") as context_f, \\\n",
    "     open(question_file_path,\"w\") as question_f, \\\n",
    "     open(answer_file_path,\"w\") as answer_f, \\\n",
    "     open(span_file_path,\"w\") as span_f:\n",
    "    \n",
    "    for data in dataset: \n",
    "        (context, question, answer, span) = data\n",
    "\n",
    "        context_f.write(context + \"\\n\") \n",
    "        question_f.write(question + \"\\n\") \n",
    "        answer_f.write(answer + \"\\n\") \n",
    "        span_f.write(span + \"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
