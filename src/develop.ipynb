{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37853986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import preprocess\n",
    "importlib.reload(preprocess)\n",
    "from preprocess import download_squad_dataset, process_split, write_to_files\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904dece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon1/dnlp2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-14 11:30:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 66.4MB/s]                    \n",
      "2025-06-14 11:30:03 INFO: Downloaded file to /home/leon1/stanza_resources/resources.json\n",
      "2025-06-14 11:30:03 WARNING: Language en package default expects mwt, which has been added\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/tokenize/combined.pt: 100%|██████████| 651k/651k [00:00<00:00, 13.1MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/mwt/combined.pt: 100%|██████████| 509k/509k [00:00<00:00, 12.1MB/s]\n",
      "2025-06-14 11:30:04 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-06-14 11:30:04 INFO: Using device: cpu\n",
      "2025-06-14 11:30:04 INFO: Loading: tokenize\n",
      "2025-06-14 11:30:05 INFO: Loading: mwt\n",
      "2025-06-14 11:30:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappingissues: 0\n",
      "spanissues: 0\n",
      "tokenissues: 0\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "eval_dataset = process_split(eval, nlp)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f14537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = process_split(train)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")\n",
    "#t_context_tokens, t_question_tokens, t_answer_tokens, t_span_tokens = write_to_files(train_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d61b",
   "metadata": {},
   "source": [
    "## Map tokens to embedding indices\n",
    "\n",
    "- load GloVe embeddings\n",
    "- map vocabulary to embedding indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048a1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leon1/dnlp2025/glove_embeddings/glove.840B.300d.txt\n",
      "Done!  400000 words loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "glove_path = os.path.abspath(os.path.dirname(os.getcwd())) + \"/glove_embeddings/glove.840B.300d.txt\"\n",
    "print(glove_path) \n",
    "assert os.path.exists(glove_path), (\"glove embeddings file missing! Please download the correct embeddings and place them into the glove_embeddings directory\")\n",
    "embedding_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        vals = line.split(' ')\n",
    "        word = vals[0]\n",
    "        coefs = np.asarray(vals[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Done! \", len(embedding_index),\"words loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed61b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_dim = 300\n",
    "embedding_matrix = []\n",
    "\n",
    "word2idx[\"[PAD]\"] = 0\n",
    "word2idx[\"[UNK]\"] = 1\n",
    "idx2word.append(\"[PAD]\")\n",
    "idx2word.append(\"[UNK]\")\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "\n",
    "def get_or_create_index(token):\n",
    "    token_lower = token.lower()\n",
    "    if(token_lower) in word2idx:\n",
    "        return word2idx[token_lower]\n",
    "    else:\n",
    "        idx = len(word2idx)\n",
    "        word2idx[token_lower] = idx\n",
    "        idx2word.append(token_lower)\n",
    "        if token_lower in embedding_index:\n",
    "            embedding_matrix.append(embedding_index[token_lower])\n",
    "        else:\n",
    "            embedding_matrix.append(np.random.normal(scale=0.01, size=embedding_dim))\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0302f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n",
      "31338\n",
      "25764\n",
      "25764\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 12, 15, 8, 16, 17, 18, 19, 20, 12, 21, 22, 23, 12, 7, 8, 24, 17, 25, 19, 13, 26, 27, 28, 12, 15, 8, 24, 17, 29, 19, 13, 30, 31, 32, 33, 34, 10, 35, 36, 37, 2, 3, 38, 23, 12, 9, 5, 39, 40, 41, 42, 43, 44, 43, 45, 46, 47, 48, 12, 49, 50, 51, 52, 45, 53, 54, 43, 55, 23, 56, 57, 5, 12, 58, 2, 3, 43, 12, 16, 59, 12, 60, 61, 62, 60, 63, 64, 65, 66, 67, 68, 43, 56, 69, 56, 70, 71, 12, 72, 14, 73, 74, 2, 3, 9, 63, 75, 76, 17, 77, 78, 12, 9, 79, 80, 81, 82, 56, 60, 2, 3, 83, 60, 19, 43, 84, 85, 12, 86, 87, 88, 89, 12, 90, 76, 4, 23]\n"
     ]
    }
   ],
   "source": [
    "sen_idxs = []\n",
    "#do this for every token in contexts,question and answers\n",
    "all_tokens = []\n",
    "all_tokens.extend(e_context_tokens)\n",
    "all_tokens.extend(e_question_tokens)\n",
    "all_tokens.extend(e_answer_tokens)\n",
    "print(len(all_tokens))\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    if tokens is None:\n",
    "        continue\n",
    "    idx = [get_or_create_index(t) for t in tokens.split()]\n",
    "    sen_idxs.append(idx)\n",
    "print(len(sen_idxs))\n",
    "print(len(word2idx))\n",
    "print(len(idx2word))\n",
    "print(sen_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9ee549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25764, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f147bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True, dropout=dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.w = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.sentinel = nn.Parameter(torch.randn(1,hidden_dim))\n",
    "\n",
    "    def encode_sequence(self, idxs, mask):\n",
    "        lengths = mask.sum(dim=1)  # [batch]\n",
    "        sorted_lens, sorted_idx = lengths.sort(descending=True)\n",
    "        _, orig_idx = sorted_idx.sort()\n",
    "\n",
    "        # Sort sequences for packing\n",
    "        idxs_sorted = idxs[sorted_idx]\n",
    "        emb = self.embedding(idxs_sorted)\n",
    "        packed = pack_padded_sequence(emb, sorted_lens.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # LSTM encoding\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [batch, max_len, hidden]\n",
    "        out = self.dropout(out)\n",
    "        out = out[orig_idx]  # restore original order\n",
    "\n",
    "        # Insert sentinel at end-of-sequence index for each example\n",
    "        batch_size = out.size(0)\n",
    "        sentinel_expanded = self.sentinel.expand(batch_size, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "\n",
    "        out_with_sentinel = torch.cat([out, torch.zeros_like(sentinel_expanded)], dim=1)  # [batch, max_len+1, hidden]\n",
    "        lens = lengths.long().unsqueeze(1).unsqueeze(2).expand(-1, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "        out_with_sentinel = out_with_sentinel.scatter(1, lens, sentinel_expanded)\n",
    "\n",
    "        return out_with_sentinel  # [batch, seq_len + 1, hidden]\n",
    "\n",
    "    def forward(self, doc_idxs, doc_mask, q_idxs, q_mask):\n",
    "        \"\"\"\n",
    "        doc_idxs/q_idxs: [batch, seq_len]\n",
    "        doc_mask/q_mask: [batch, seq_len]\n",
    "        \"\"\"\n",
    "        D = self.encode_sequence(doc_idxs, doc_mask)  # [batch, m+1, hidden]\n",
    "        Q_prime = self.encode_sequence(q_idxs, q_mask)  # [batch, n+1, hidden]\n",
    "\n",
    "        # Nonlinear projection: Q = tanh(W * Q′ + b)\n",
    "        Q = torch.tanh(self.w(Q_prime) + self.b)  # [batch, n+1, hidden]\n",
    "\n",
    "        return D, Q       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9b422",
   "metadata": {},
   "source": [
    "## Small Encoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e68f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document encoding shape: torch.Size([1, 6, 64])\n",
      "Question encoding shape: torch.Size([1, 3, 64])\n",
      "\n",
      "Sentinel vector (document): tensor([-0.5178, -1.9964, -0.0055, -0.1947, -0.0296,  2.8728, -0.9718,  0.7252,\n",
      "         2.3006,  0.6986, -0.7614, -0.5578,  0.3154, -0.3911, -0.2983,  1.4246,\n",
      "        -0.8788, -1.3894, -0.9157, -0.4858,  0.7712,  1.7142,  0.4245,  0.9347,\n",
      "        -0.0751,  1.4394,  0.1151,  0.8613,  0.0950,  1.2354, -0.3196, -0.8543,\n",
      "        -0.8129, -0.0994,  0.9303,  1.2679,  1.1180, -1.1823, -0.5686, -0.1963,\n",
      "         2.7319,  1.0414,  0.4065,  0.1175, -0.1640, -1.2357,  2.2861, -0.3782,\n",
      "        -0.7519, -1.9009,  0.2609, -1.7670, -1.3268, -0.6697,  2.0614, -1.7673,\n",
      "         0.0242, -0.9860,  1.3433, -0.2099, -0.4606,  1.2304,  0.2368, -0.2514],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Sentinel vector (question): tensor([ 0.7315, -0.2916, -0.7098,  0.7575, -0.3021,  0.4009,  0.6846, -0.2770,\n",
      "        -0.6569, -0.6911, -0.7479,  0.0341,  0.1462,  0.6462, -0.0092, -0.4113,\n",
      "         0.2393,  0.7979,  0.3356,  0.4745,  0.1079,  0.6447,  0.6820,  0.7076,\n",
      "        -0.6110, -0.7238, -0.5261, -0.0445,  0.9079, -0.6773,  0.3155,  0.2545,\n",
      "        -0.5541,  0.0432, -0.3898, -0.3074, -0.0474,  0.5024, -0.7536,  0.8347,\n",
      "        -0.8862, -0.3633,  0.6923, -0.5102,  0.4002, -0.5707, -0.6523,  0.8266,\n",
      "         0.9098,  0.5089, -0.6459, -0.3850, -0.4046, -0.2392,  0.0878,  0.1594,\n",
      "        -0.6936, -0.5181, -0.4777,  0.2755,  0.9282, -0.5600,  0.1530, -0.4586],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Dummy vocab\n",
    "word2idx_test = {\n",
    "    \"[PAD]\": 0, \"[UNK]\": 1,\n",
    "    \"the\": 2, \"quick\": 3, \"brown\": 4, \"fox\": 5, \"jumps\": 6, \"over\": 7, \"lazy\": 8, \"dog\": 9\n",
    "}\n",
    "\n",
    "# Random embedding matrix for vocab (vocab_size x emb_dim)\n",
    "vocab_size = len(word2idx_test)\n",
    "embedding_dim = 50\n",
    "embedding_matrix_test = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Dummy inputs\n",
    "# Document: \"the quick brown fox jumps\"\n",
    "# Question: \"the fox\"\n",
    "doc_tokens = [2, 3, 4, 5, 6]\n",
    "q_tokens = [2, 5]\n",
    "\n",
    "# Padding to max length\n",
    "doc_max_len = 6\n",
    "q_max_len = 4\n",
    "doc_input = [doc_tokens + [0] * (doc_max_len - len(doc_tokens))]  # batch size 1\n",
    "q_input = [q_tokens + [0] * (q_max_len - len(q_tokens))]\n",
    "\n",
    "# Masks (1 for real tokens, 0 for padding)\n",
    "doc_mask = [[1]*len(doc_tokens) + [0]*(doc_max_len - len(doc_tokens))]\n",
    "q_mask = [[1]*len(q_tokens) + [0]*(q_max_len - len(q_tokens))]\n",
    "\n",
    "# Convert to tensors\n",
    "doc_idxs = torch.tensor(doc_input)      # [1, 6]\n",
    "doc_mask = torch.tensor(doc_mask)       # [1, 6]\n",
    "q_idxs = torch.tensor(q_input)          # [1, 4]\n",
    "q_mask = torch.tensor(q_mask)           # [1, 4]\n",
    "\n",
    "hidden_size = 64\n",
    "encoder = Encoder(hidden_size, embedding_matrix_test, 0)\n",
    "\n",
    "# Run encoder\n",
    "D, Q = encoder(doc_idxs, doc_mask, q_idxs, q_mask)\n",
    "\n",
    "# Outputs\n",
    "print(\"Document encoding shape:\", D.shape)  # [1, m+1, 64]\n",
    "print(\"Question encoding shape:\", Q.shape)      # [1, n+1, 64]\n",
    "\n",
    "print(\"\\nSentinel vector (document):\", D[0, -1])\n",
    "print(\"Sentinel vector (question):\", Q[0, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be400c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as functional\n",
    "\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out\n",
    "\n",
    "class CoattentionEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.brnn = BRNN(input_size=3 * hidden_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         num_layers=num_layers)\n",
    "\n",
    "    #2.2 Coattention Encoder\n",
    "    def forward(self, D, Q):\n",
    "        #Affinity matrix\n",
    "        L = torch.bmm(Q, torch.transpose(D, 1, 2)) \n",
    "\n",
    "        #Attention weights\n",
    "        AQ = functional.softmax(L, dim=1)         \n",
    "        AD = functional.softmax(torch.transpose(L, 1, 2), dim=1)  \n",
    "\n",
    "        #Context Summaries\n",
    "        CQ = torch.bmm(AQ, D) \n",
    "        Q_combined = torch.cat([Q, CQ], dim=2)   \n",
    "        CD = torch.bmm(AD, Q_combined)\n",
    "\n",
    "        #BRNN\n",
    "        return self.brnn(torch.cat([D, CD], dim=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dbd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.8170e-02,  3.0651e-03, -1.6778e-02, -3.4676e-02, -3.1467e-02,\n",
      "           2.7263e-03, -1.0733e-02,  3.6497e-02,  6.5026e-02,  2.1943e-02,\n",
      "           4.1403e-02, -6.9917e-03,  6.4237e-02,  1.0203e-01, -1.5333e-02,\n",
      "           2.2456e-02, -3.4000e-02,  1.9732e-03,  3.8344e-02, -8.9095e-02,\n",
      "           2.2353e-03, -2.6979e-03,  1.5462e-02,  3.8507e-02, -6.0910e-02,\n",
      "           2.6456e-02, -3.0254e-02, -5.7672e-02,  5.9515e-02,  8.8467e-03,\n",
      "          -7.0999e-02,  7.2277e-02, -1.2743e-02,  5.5897e-02,  3.0679e-02,\n",
      "          -2.8931e-02, -1.3734e-02, -4.9860e-03, -2.1400e-03,  1.7617e-02,\n",
      "           1.5752e-02,  3.0865e-02, -2.8231e-02, -3.9334e-02, -8.9450e-02,\n",
      "          -2.5320e-02, -8.4639e-02,  2.7392e-02,  6.7270e-02,  4.6448e-02,\n",
      "           1.8610e-03,  2.2511e-03, -2.6907e-02, -4.2986e-02, -1.0375e-02,\n",
      "           1.3769e-03,  2.1865e-02, -5.9250e-02,  3.6065e-02, -6.6762e-02,\n",
      "           2.3081e-02, -6.9961e-02,  1.9613e-02,  9.0126e-03,  2.9161e-02,\n",
      "           4.5725e-02, -2.5366e-02, -5.6503e-04,  7.7586e-02, -8.0467e-02,\n",
      "          -1.1037e-01, -1.4752e-01, -7.2651e-02, -4.8870e-02, -1.0315e-01,\n",
      "          -6.6193e-03, -9.1492e-03,  1.8563e-02, -7.6979e-02, -1.1798e-01,\n",
      "          -1.2475e-01, -2.2628e-02,  5.4666e-02,  2.9645e-02,  1.3631e-01,\n",
      "           7.4056e-02,  9.6061e-02,  7.3263e-02, -1.5741e-01, -1.1760e-02,\n",
      "           4.5961e-02,  5.0424e-02, -4.5464e-02, -5.2196e-02, -4.2900e-02,\n",
      "           1.6449e-02, -1.1316e-01,  1.3847e-01, -7.1343e-02, -6.9086e-03,\n",
      "          -5.6867e-02,  8.9007e-02,  5.9510e-02,  1.8829e-02,  2.2901e-02,\n",
      "          -4.5899e-02, -8.3195e-03,  4.1910e-03, -7.6745e-02,  5.1915e-03,\n",
      "          -3.2620e-02, -1.8129e-01, -1.9684e-02, -4.1662e-02,  5.1385e-02,\n",
      "          -5.7690e-02, -1.2088e-01, -7.7793e-02,  4.0037e-02, -1.0889e-01,\n",
      "           1.2719e-02,  1.2854e-01,  3.7960e-02,  2.0872e-02,  1.9133e-03,\n",
      "          -1.3980e-02, -2.1587e-01, -1.1930e-01],\n",
      "         [-5.3936e-02,  5.0539e-03, -2.3295e-02, -5.0772e-02, -4.9152e-02,\n",
      "          -1.0759e-04, -1.6046e-02,  5.8530e-02,  8.0324e-02,  3.2903e-02,\n",
      "           5.6615e-02, -1.6255e-02,  9.9676e-02,  1.4488e-01, -2.1709e-02,\n",
      "           3.1890e-02, -5.6677e-02,  2.8098e-03,  4.7899e-02, -1.2898e-01,\n",
      "           1.3602e-02,  4.4791e-03,  2.5046e-02,  5.6864e-02, -9.0060e-02,\n",
      "           3.1011e-02, -5.2753e-02, -7.9951e-02,  8.1669e-02,  1.3211e-02,\n",
      "          -1.0901e-01,  1.1693e-01, -1.7850e-02,  8.4855e-02,  5.5288e-02,\n",
      "          -2.6099e-02, -1.9152e-02, -8.8428e-03, -4.9712e-03,  3.1271e-02,\n",
      "           2.0228e-02,  5.9195e-02, -3.6744e-02, -6.3785e-02, -1.2682e-01,\n",
      "          -3.5419e-02, -1.2502e-01,  3.9959e-02,  9.8395e-02,  7.9429e-02,\n",
      "           4.8905e-03, -1.0399e-02, -3.9976e-02, -6.4336e-02, -1.6135e-02,\n",
      "          -4.0598e-03,  3.1854e-02, -9.6405e-02,  4.9179e-02, -9.9796e-02,\n",
      "           3.7682e-02, -1.0003e-01,  1.7267e-02,  1.7725e-02,  2.6441e-02,\n",
      "           5.2377e-02, -2.3884e-02,  9.1297e-03,  6.6914e-02, -8.8854e-02,\n",
      "          -1.0562e-01, -1.5235e-01, -6.6673e-02, -4.3593e-02, -9.5810e-02,\n",
      "          -1.8504e-02, -1.1628e-02,  2.3747e-02, -6.6027e-02, -1.2218e-01,\n",
      "          -1.3799e-01, -2.2989e-02,  5.7561e-02,  3.6641e-02,  1.3118e-01,\n",
      "           7.3353e-02,  1.0187e-01,  8.0687e-02, -1.3834e-01, -1.1970e-02,\n",
      "           3.2922e-02,  5.8219e-02, -4.1707e-02, -4.4000e-02, -3.0480e-02,\n",
      "           9.9261e-03, -1.0122e-01,  1.3596e-01, -8.3935e-02,  6.0385e-03,\n",
      "          -5.5564e-02,  8.1644e-02,  6.5731e-02,  4.3520e-02,  4.1134e-02,\n",
      "          -3.7005e-02, -5.1280e-03,  3.0874e-04, -8.8928e-02,  1.1310e-02,\n",
      "          -3.3643e-02, -1.7165e-01, -1.0594e-02, -4.0769e-02,  4.9756e-02,\n",
      "          -5.6590e-02, -1.1957e-01, -8.2384e-02,  3.9018e-02, -1.2242e-01,\n",
      "           1.7038e-02,  1.3230e-01,  3.0043e-02,  2.0563e-02,  1.3713e-02,\n",
      "          -2.9975e-02, -2.1966e-01, -1.2476e-01],\n",
      "         [-5.9791e-02,  6.6179e-03, -2.4894e-02, -5.7659e-02, -6.2586e-02,\n",
      "          -2.9681e-03, -2.0056e-02,  6.9264e-02,  8.0830e-02,  3.8030e-02,\n",
      "           6.3158e-02, -2.3669e-02,  1.1854e-01,  1.6300e-01, -2.1705e-02,\n",
      "           3.6226e-02, -7.1094e-02,  2.1441e-03,  5.1395e-02, -1.4836e-01,\n",
      "           2.5823e-02,  1.1649e-02,  2.9915e-02,  6.8775e-02, -1.0541e-01,\n",
      "           3.1502e-02, -6.6827e-02, -8.6998e-02,  9.1865e-02,  2.1426e-02,\n",
      "          -1.2985e-01,  1.4129e-01, -1.9227e-02,  1.0016e-01,  7.3336e-02,\n",
      "          -1.9627e-02, -2.1598e-02, -1.1394e-02, -6.7657e-03,  4.0224e-02,\n",
      "           2.3027e-02,  7.7451e-02, -3.7302e-02, -7.8825e-02, -1.4521e-01,\n",
      "          -3.5453e-02, -1.4460e-01,  4.5386e-02,  1.1233e-01,  9.5059e-02,\n",
      "           4.5780e-03, -2.4485e-02, -5.0748e-02, -7.4905e-02, -2.0998e-02,\n",
      "          -7.7443e-03,  3.6721e-02, -1.1595e-01,  5.6240e-02, -1.1714e-01,\n",
      "           4.7965e-02, -1.1352e-01,  1.4864e-02,  2.1907e-02,  2.2122e-02,\n",
      "           5.4220e-02, -2.6429e-02,  2.4778e-02,  5.1893e-02, -9.8576e-02,\n",
      "          -1.0275e-01, -1.5826e-01, -5.0991e-02, -3.4851e-02, -8.6417e-02,\n",
      "          -3.5121e-02, -1.1154e-02,  2.2032e-02, -4.6794e-02, -1.2885e-01,\n",
      "          -1.4996e-01, -2.5230e-02,  5.8660e-02,  4.5953e-02,  1.2320e-01,\n",
      "           7.6334e-02,  1.0881e-01,  8.4417e-02, -1.0324e-01, -1.3585e-02,\n",
      "           1.5331e-02,  7.0294e-02, -3.5017e-02, -2.9605e-02, -1.2310e-02,\n",
      "          -3.1458e-03, -8.2246e-02,  1.2296e-01, -9.7445e-02,  1.9706e-02,\n",
      "          -5.8098e-02,  6.1023e-02,  7.2049e-02,  7.6246e-02,  6.6358e-02,\n",
      "          -2.0796e-02, -4.0902e-03, -8.9244e-03, -1.0954e-01,  2.9075e-02,\n",
      "          -2.7763e-02, -1.5377e-01,  7.5667e-03, -4.2191e-02,  4.3519e-02,\n",
      "          -5.3078e-02, -1.1876e-01, -8.3018e-02,  3.8615e-02, -1.3382e-01,\n",
      "           2.1349e-02,  1.3946e-01,  1.9664e-02,  2.3679e-02,  2.7394e-02,\n",
      "          -5.3402e-02, -2.1784e-01, -1.2858e-01],\n",
      "         [-6.1911e-02,  7.8115e-03, -2.5177e-02, -6.0757e-02, -7.0392e-02,\n",
      "          -5.2759e-03, -2.3206e-02,  7.3744e-02,  7.7916e-02,  4.2750e-02,\n",
      "           6.7060e-02, -3.0947e-02,  1.2839e-01,  1.7192e-01, -1.9759e-02,\n",
      "           3.6055e-02, -7.7688e-02,  2.3129e-03,  5.3671e-02, -1.5921e-01,\n",
      "           3.3151e-02,  1.6889e-02,  3.1346e-02,  7.5197e-02, -1.1355e-01,\n",
      "           3.0030e-02, -7.3323e-02, -8.7364e-02,  9.4502e-02,  2.5816e-02,\n",
      "          -1.3841e-01,  1.5283e-01, -1.9868e-02,  1.0907e-01,  8.2686e-02,\n",
      "          -1.4181e-02, -2.4476e-02, -1.2864e-02, -8.7130e-03,  4.5370e-02,\n",
      "           2.2661e-02,  8.9850e-02, -3.5233e-02, -8.5708e-02, -1.5552e-01,\n",
      "          -3.1812e-02, -1.5320e-01,  4.9797e-02,  1.1906e-01,  1.0459e-01,\n",
      "           5.1155e-03, -3.5361e-02, -6.0981e-02, -8.2007e-02, -2.4322e-02,\n",
      "          -1.3265e-02,  3.6508e-02, -1.2560e-01,  5.9238e-02, -1.2518e-01,\n",
      "           5.4350e-02, -1.2109e-01,  1.4002e-02,  2.2498e-02,  1.3737e-02,\n",
      "           5.0369e-02, -3.4429e-02,  5.3892e-02,  2.8675e-02, -1.1292e-01,\n",
      "          -1.0167e-01, -1.6778e-01, -2.1973e-02, -1.7357e-02, -6.2489e-02,\n",
      "          -5.8480e-02, -8.1578e-03,  1.1736e-02, -1.4967e-02, -1.2990e-01,\n",
      "          -1.5959e-01, -2.8313e-02,  5.7417e-02,  6.0026e-02,  1.0943e-01,\n",
      "           8.3599e-02,  1.2195e-01,  8.6044e-02, -4.6942e-02, -1.9028e-02,\n",
      "          -5.1916e-03,  8.6690e-02, -2.9623e-02, -1.1960e-02,  2.0971e-02,\n",
      "          -2.5420e-02, -5.5113e-02,  9.6407e-02, -1.0978e-01,  3.9961e-02,\n",
      "          -6.4810e-02,  2.3772e-02,  7.3008e-02,  1.1494e-01,  1.0432e-01,\n",
      "           1.2278e-02, -4.4750e-03, -2.5772e-02, -1.3729e-01,  6.8110e-02,\n",
      "          -1.4358e-02, -1.1076e-01,  4.1193e-02, -4.5347e-02,  4.2538e-02,\n",
      "          -5.0859e-02, -1.1167e-01, -8.0369e-02,  3.5176e-02, -1.4280e-01,\n",
      "           2.9933e-02,  1.4942e-01, -1.1924e-03,  3.3087e-02,  4.8906e-02,\n",
      "          -8.9021e-02, -2.0568e-01, -1.2689e-01],\n",
      "         [-6.2876e-02,  8.9663e-03, -2.4803e-02, -6.3297e-02, -7.6102e-02,\n",
      "          -7.0082e-03, -2.7244e-02,  7.5338e-02,  7.3756e-02,  4.3052e-02,\n",
      "           6.8286e-02, -3.4482e-02,  1.3336e-01,  1.7489e-01, -1.7135e-02,\n",
      "           3.5340e-02, -8.3238e-02,  2.8834e-03,  5.4025e-02, -1.6589e-01,\n",
      "           3.8535e-02,  2.1343e-02,  3.2525e-02,  7.8895e-02, -1.1874e-01,\n",
      "           2.8575e-02, -7.5687e-02, -8.9892e-02,  9.6999e-02,  2.6161e-02,\n",
      "          -1.4162e-01,  1.5848e-01, -1.9722e-02,  1.1292e-01,  8.9387e-02,\n",
      "          -1.2313e-02, -2.7153e-02, -1.3883e-02, -1.0083e-02,  4.8648e-02,\n",
      "           2.3533e-02,  9.7259e-02, -3.4347e-02, -8.6435e-02, -1.6101e-01,\n",
      "          -3.0118e-02, -1.5854e-01,  5.2697e-02,  1.2213e-01,  1.0742e-01,\n",
      "           4.6829e-03, -4.1992e-02, -6.4214e-02, -8.6964e-02, -2.7131e-02,\n",
      "          -1.6337e-02,  3.6730e-02, -1.3338e-01,  6.0320e-02, -1.3052e-01,\n",
      "           5.9034e-02, -1.2400e-01,  1.1875e-02,  2.2880e-02, -1.1214e-02,\n",
      "           7.4247e-03, -5.6498e-02,  9.7878e-02,  1.0231e-03, -1.4256e-01,\n",
      "          -1.0581e-01, -1.8525e-01,  3.9195e-02,  1.2386e-02, -1.1809e-02,\n",
      "          -8.7283e-02, -6.6600e-03, -3.7371e-02,  4.8622e-02, -1.1337e-01,\n",
      "          -1.5438e-01, -3.2343e-02,  5.5194e-02,  8.1435e-02,  9.6953e-02,\n",
      "           1.0838e-01,  1.4210e-01,  7.2344e-02,  3.9085e-02, -2.6816e-02,\n",
      "          -1.4507e-02,  1.0609e-01, -2.7081e-02,  2.6056e-03,  8.1886e-02,\n",
      "          -5.9878e-02, -2.0037e-02,  4.1067e-02, -1.1747e-01,  8.0891e-02,\n",
      "          -8.7984e-02, -4.8081e-02,  5.7719e-02,  1.5854e-01,  1.5906e-01,\n",
      "           7.5690e-02, -6.4828e-03, -6.1637e-02, -1.7688e-01,  1.4182e-01,\n",
      "           1.1563e-02, -1.4318e-02,  1.0011e-01, -4.0826e-02,  5.5519e-02,\n",
      "          -5.1409e-02, -8.9894e-02, -7.1469e-02,  2.5912e-02, -1.4405e-01,\n",
      "           4.8464e-02,  1.6472e-01, -4.1887e-02,  5.4380e-02,  7.7395e-02,\n",
      "          -1.4474e-01, -1.6381e-01, -1.2004e-01],\n",
      "         [ 1.1172e-01, -1.7587e-02,  6.0648e-03, -2.5278e-02, -8.1314e-02,\n",
      "          -5.4033e-02,  9.5537e-02, -1.2234e-01,  7.5447e-02,  6.5295e-02,\n",
      "           4.0631e-02,  3.4578e-03,  2.2250e-01,  1.7116e-01, -4.9680e-02,\n",
      "           3.6814e-02, -2.0204e-01, -9.9391e-02,  7.4207e-02, -3.2296e-02,\n",
      "          -1.9797e-02, -1.5035e-01, -1.2674e-01,  1.1476e-01, -1.2354e-01,\n",
      "           7.5733e-02,  5.4233e-02, -1.8557e-02,  1.7736e-01,  1.9362e-01,\n",
      "          -1.0499e-01,  6.4267e-02,  7.6768e-02,  1.6169e-01,  9.2798e-02,\n",
      "           1.8995e-03,  3.4960e-01,  2.4374e-03, -1.6826e-01, -1.7837e-01,\n",
      "           2.3348e-01,  1.8850e-01, -1.0571e-01, -4.4552e-02, -1.9975e-01,\n",
      "          -5.8952e-02, -2.5359e-01,  1.5157e-01,  3.0487e-01,  2.5875e-02,\n",
      "           1.7559e-02, -9.9306e-03, -2.7396e-02,  3.9066e-02,  2.2002e-02,\n",
      "           2.2938e-01,  1.9333e-01,  6.6174e-03,  4.2363e-02,  9.6559e-03,\n",
      "          -3.1091e-02,  1.1579e-01, -1.4099e-01,  1.9442e-01, -3.8126e-02,\n",
      "          -2.1543e-01, -1.3789e-01,  1.3470e-01, -2.1629e-02, -7.6732e-02,\n",
      "          -2.2096e-01, -2.1697e-01,  2.4120e-01,  5.4806e-02,  1.0386e-01,\n",
      "          -2.3578e-01, -6.9506e-03, -1.4855e-01,  1.6098e-01, -2.5043e-02,\n",
      "          -5.2569e-02, -3.6171e-02,  6.6736e-02,  1.0493e-01,  8.6352e-02,\n",
      "           1.7825e-01,  2.3240e-01, -2.5702e-02,  1.7787e-01, -3.1230e-02,\n",
      "           4.2048e-02,  2.1088e-01,  3.4328e-03, -4.6903e-02,  1.6516e-01,\n",
      "          -1.6921e-01,  2.9181e-02, -9.3499e-02, -8.7758e-02,  9.3193e-02,\n",
      "          -1.2124e-01, -1.6988e-01,  1.3734e-02,  2.1186e-01,  2.4063e-01,\n",
      "           2.1976e-01, -1.6003e-02, -2.0545e-01, -2.4631e-01,  3.7111e-01,\n",
      "           7.0789e-02,  2.3535e-01,  2.1526e-01, -5.2576e-03,  8.5974e-02,\n",
      "          -4.8177e-02, -3.6339e-02, -3.5637e-02,  9.6284e-03, -1.8194e-01,\n",
      "           4.9352e-02,  2.7083e-01, -1.2303e-01,  1.0458e-01,  1.3967e-01,\n",
      "          -2.8279e-01, -6.5482e-02, -1.8665e-01]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "coattention_enc = CoattentionEncoder(hidden_size).to(device)\n",
    "\n",
    "U = coattention_enc(D, Q)  \n",
    "\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84639d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
