{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37853986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/workspace/dnlp2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocess import download_squad_dataset\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831521a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 19:41:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 26.7MB/s]                    \n",
      "2025-06-11 19:41:14 INFO: Downloaded file to /home/luca/stanza_resources/resources.json\n",
      "2025-06-11 19:41:14 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-11 19:41:14 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-06-11 19:41:14 INFO: Using device: cpu\n",
      "2025-06-11 19:41:14 INFO: Loading: tokenize\n",
      "2025-06-11 19:41:16 INFO: Loading: mwt\n",
      "2025-06-11 19:41:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for sen in doc.sentences:\n",
    "        for token in sen.tokens:\n",
    "            tokens.append(token.text)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecf6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapCharToToken(context, tokens):\n",
    "\n",
    "    concat = \"\"\n",
    "    curr = 0\n",
    "    mapping = {}\n",
    "\n",
    "    for i, char in enumerate(context):\n",
    "        if char != ' ' and char != '\\n':\n",
    "            concat += char\n",
    "            ctoken = tokens[curr]\n",
    "            if concat == ctoken:\n",
    "                start = i - len(concat) + 1\n",
    "                for loc in range(start, i+1):\n",
    "                    mapping[loc] = (concat, curr)\n",
    "                concat = \"\"\n",
    "                curr += 1\n",
    "    if curr != len(tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da811973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappingissues: 0\n",
      "spanissues: 0\n",
      "tokenissues: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "i = 0\n",
    "mappingissues = 0\n",
    "spanissues = 0\n",
    "tokenissues = 0\n",
    "dataset = []\n",
    "\n",
    "for article in eval[\"data\"]:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        context = paragraph[\"context\"]\n",
    "        context.replace(\"''\",'\" ') \n",
    "        context.replace(\"``\",'\" ') \n",
    "        context_tokens = tokenize(context)\n",
    "        context = context.lower()\n",
    "\n",
    "        mapping = mapCharToToken(context, context_tokens)\n",
    "\n",
    "        if mapping is None:\n",
    "            mappingissues += 1\n",
    "            print(article[\"title\"])\n",
    "            continue\n",
    "        \n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            question_tokens = tokenize(qa[\"question\"])\n",
    "\n",
    "            answer_text = qa[\"answers\"][0][\"text\"].lower()\n",
    "            answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            if context[answer_start:answer_end] != answer_text:\n",
    "                spanissues += 1\n",
    "                continue\n",
    "\n",
    "            answer_start_wordloc = mapping[answer_start][1]\n",
    "            answer_end_wordloc = mapping[answer_end-1][1]\n",
    "\n",
    "            answer_tokens = context_tokens[answer_start_wordloc:answer_end_wordloc+1]\n",
    "\n",
    "            if \"\".join(answer_tokens) != \"\".join(answer_text.split()):\n",
    "                tokenissues += 1\n",
    "                continue\n",
    "            dataset.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(answer_tokens), ' '.join([str(answer_start_wordloc), str(answer_end_wordloc)])))\n",
    "\n",
    "print(f\"mappingissues: {mappingissues}\")\n",
    "print(f\"spanissues: {spanissues}\")\n",
    "print(f\"tokenissues: {mappingissues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "94f14537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "\n",
    "context_file_path = os.path.join(current_dir, \"data.context\")\n",
    "question_file_path = os.path.join(current_dir, \"data.question\")\n",
    "answer_file_path = os.path.join(current_dir, \"data.answer\")\n",
    "span_file_path = os.path.join(current_dir, \"data.span\")\n",
    "\n",
    "context_tokens = []\n",
    "question_tokens = []\n",
    "answer_tokens = []\n",
    "span_tokens = []\n",
    "\n",
    "with open(context_file_path,\"w\") as context_f, \\\n",
    "     open(question_file_path,\"w\") as question_f, \\\n",
    "     open(answer_file_path,\"w\") as answer_f, \\\n",
    "     open(span_file_path,\"w\") as span_f:\n",
    "    \n",
    "    for data in dataset: \n",
    "        (context, question, answer, span) = data\n",
    "\n",
    "        context_f.write(context + \"\\n\") \n",
    "        question_f.write(question + \"\\n\") \n",
    "        answer_f.write(answer + \"\\n\") \n",
    "        span_f.write(span + \"\\n\") \n",
    "\n",
    "        context_tokens.append(context)\n",
    "        question_tokens.append(question)\n",
    "        answer_tokens.append(answer)\n",
    "        span_tokens.append(span)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d61b",
   "metadata": {},
   "source": [
    "## Map tokens to embedding indices\n",
    "\n",
    "- load GloVe embeddings\n",
    "- map vocabulary to embedding indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "048a1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luca/workspace/dnlp2025/glove_embeddings/glove.840B.300d.txt\n",
      "Done!  2196016 words loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "glove_path = os.path.abspath(os.path.dirname(os.getcwd())) + \"/glove_embeddings/glove.840B.300d.txt\"\n",
    "print(glove_path) \n",
    "assert os.path.exists(glove_path), (\"glove embeddings missing!\")\n",
    "embedding_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        vals = line.split(' ')\n",
    "        word = vals[0]\n",
    "        coefs = np.asarray(vals[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Done! \", len(embedding_index),\"words loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed61b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_dim = 300\n",
    "embedding_matrix = []\n",
    "\n",
    "word2idx[\"[UNK]\"] = 0\n",
    "idx2word.append(\"[UNK]\")\n",
    "word2idx[\"[PAD]\"] = 1\n",
    "idx2word.append(\"[PAD]\")\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "\n",
    "def get_or_create_index(token):\n",
    "    token_lower = token.lower()\n",
    "    if(token_lower) in word2idx:\n",
    "        return word2idx[token_lower]\n",
    "    else:\n",
    "        idx = len(word2idx)\n",
    "        word2idx[token_lower] = idx\n",
    "        idx2word.append(token_lower)\n",
    "        if token_lower in embedding_index:\n",
    "            embedding_matrix.append(embedding_index[token_lower])\n",
    "        else:\n",
    "            embedding_matrix.append(np.random.normal(scale=0.01, size=embedding_dim))\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0302f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n",
      "25764\n",
      "25764\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "#do this for every token in contexts,question and answers\n",
    "all_tokens = []\n",
    "all_tokens.extend(context_tokens)\n",
    "all_tokens.extend(question_tokens)\n",
    "all_tokens.extend(answer_tokens)\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    if tokens is None:\n",
    "        continue\n",
    "    idx = [get_or_create_index(t) for t in tokens.split()]\n",
    "    idxs.append(idx)\n",
    "print(len(idxs))\n",
    "print(len(word2idx))\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c9ee549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25764, 300)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "embedding_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
