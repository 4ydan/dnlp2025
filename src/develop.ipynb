{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37853986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import preprocess\n",
    "importlib.reload(preprocess)\n",
    "from preprocess import download_squad_dataset, process_split, write_to_files\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "904dece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/dnlp2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-13 14:44:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 29.6MB/s]                    \n",
      "2025-06-13 14:44:45 INFO: Downloaded file to /home/leon/stanza_resources/resources.json\n",
      "2025-06-13 14:44:45 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-13 14:44:45 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-06-13 14:44:45 INFO: Using device: cuda\n",
      "2025-06-13 14:44:45 INFO: Loading: tokenize\n",
      "2025-06-13 14:44:47 INFO: Loading: mwt\n",
      "2025-06-13 14:44:47 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappingissues: 0\n",
      "spanissues: 0\n",
      "tokenissues: 0\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "eval_dataset = process_split(eval, nlp)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f14537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = process_split(train)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")\n",
    "#t_context_tokens, t_question_tokens, t_answer_tokens, t_span_tokens = write_to_files(train_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d61b",
   "metadata": {},
   "source": [
    "## Map tokens to embedding indices\n",
    "\n",
    "- load GloVe embeddings\n",
    "- map vocabulary to embedding indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048a1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leon/dnlp2025/glove_embeddings/glove.840B.300d.txt\n",
      "Done!  400000 words loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "glove_path = os.path.abspath(os.path.dirname(os.getcwd())) + \"/glove_embeddings/glove.840B.300d.txt\"\n",
    "print(glove_path) \n",
    "assert os.path.exists(glove_path), (\"glove embeddings file missing! Please download the correct embeddings and place them into the glove_embeddings directory\")\n",
    "embedding_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        vals = line.split(' ')\n",
    "        word = vals[0]\n",
    "        coefs = np.asarray(vals[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Done! \", len(embedding_index),\"words loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed61b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_dim = 300\n",
    "embedding_matrix = []\n",
    "\n",
    "word2idx[\"[PAD]\"] = 0\n",
    "word2idx[\"[UNK]\"] = 1\n",
    "idx2word.append(\"[PAD]\")\n",
    "idx2word.append(\"[UNK]\")\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "\n",
    "def get_or_create_index(token):\n",
    "    token_lower = token.lower()\n",
    "    if(token_lower) in word2idx:\n",
    "        return word2idx[token_lower]\n",
    "    else:\n",
    "        idx = len(word2idx)\n",
    "        word2idx[token_lower] = idx\n",
    "        idx2word.append(token_lower)\n",
    "        if token_lower in embedding_index:\n",
    "            embedding_matrix.append(embedding_index[token_lower])\n",
    "        else:\n",
    "            embedding_matrix.append(np.random.normal(scale=0.01, size=embedding_dim))\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0302f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n",
      "31338\n",
      "25764\n",
      "25764\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 12, 15, 8, 16, 17, 18, 19, 20, 12, 21, 22, 23, 12, 7, 8, 24, 17, 25, 19, 13, 26, 27, 28, 12, 15, 8, 24, 17, 29, 19, 13, 30, 31, 32, 33, 34, 10, 35, 36, 37, 2, 3, 38, 23, 12, 9, 5, 39, 40, 41, 42, 43, 44, 43, 45, 46, 47, 48, 12, 49, 50, 51, 52, 45, 53, 54, 43, 55, 23, 56, 57, 5, 12, 58, 2, 3, 43, 12, 16, 59, 12, 60, 61, 62, 60, 63, 64, 65, 66, 67, 68, 43, 56, 69, 56, 70, 71, 12, 72, 14, 73, 74, 2, 3, 9, 63, 75, 76, 17, 77, 78, 12, 9, 79, 80, 81, 82, 56, 60, 2, 3, 83, 60, 19, 43, 84, 85, 12, 86, 87, 88, 89, 12, 90, 76, 4, 23]\n"
     ]
    }
   ],
   "source": [
    "sen_idxs = []\n",
    "#do this for every token in contexts,question and answers\n",
    "all_tokens = []\n",
    "all_tokens.extend(e_context_tokens)\n",
    "all_tokens.extend(e_question_tokens)\n",
    "all_tokens.extend(e_answer_tokens)\n",
    "print(len(all_tokens))\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    if tokens is None:\n",
    "        continue\n",
    "    idx = [get_or_create_index(t) for t in tokens.split()]\n",
    "    sen_idxs.append(idx)\n",
    "print(len(sen_idxs))\n",
    "print(len(word2idx))\n",
    "print(len(idx2word))\n",
    "print(sen_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9ee549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25764, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f147bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True, dropout=dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.w = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.sentinel = nn.Parameter(torch.randn(1,hidden_dim))\n",
    "\n",
    "    def encode_sequence(self, idxs, mask):\n",
    "        lengths = mask.sum(dim=1)  # [batch]\n",
    "        sorted_lens, sorted_idx = lengths.sort(descending=True)\n",
    "        _, orig_idx = sorted_idx.sort()\n",
    "\n",
    "        # Sort sequences for packing\n",
    "        idxs_sorted = idxs[sorted_idx]\n",
    "        emb = self.embedding(idxs_sorted)\n",
    "        packed = pack_padded_sequence(emb, sorted_lens.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # LSTM encoding\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [batch, max_len, hidden]\n",
    "        out = self.dropout(out)\n",
    "        out = out[orig_idx]  # restore original order\n",
    "\n",
    "        # Insert sentinel at end-of-sequence index for each example\n",
    "        batch_size = out.size(0)\n",
    "        sentinel_expanded = self.sentinel.expand(batch_size, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "\n",
    "        out_with_sentinel = torch.cat([out, torch.zeros_like(sentinel_expanded)], dim=1)  # [batch, max_len+1, hidden]\n",
    "        lens = lengths.long().unsqueeze(1).unsqueeze(2).expand(-1, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "        out_with_sentinel = out_with_sentinel.scatter(1, lens, sentinel_expanded)\n",
    "\n",
    "        return out_with_sentinel  # [batch, seq_len + 1, hidden]\n",
    "\n",
    "    def forward(self, doc_idxs, doc_mask, q_idxs, q_mask):\n",
    "        \"\"\"\n",
    "        doc_idxs/q_idxs: [batch, seq_len]\n",
    "        doc_mask/q_mask: [batch, seq_len]\n",
    "        \"\"\"\n",
    "        D = self.encode_sequence(doc_idxs, doc_mask)  # [batch, m+1, hidden]\n",
    "        Q_prime = self.encode_sequence(q_idxs, q_mask)  # [batch, n+1, hidden]\n",
    "\n",
    "        # Nonlinear projection: Q = tanh(W * Qâ€² + b)\n",
    "        Q = torch.tanh(self.w(Q_prime) + self.b)  # [batch, n+1, hidden]\n",
    "\n",
    "        return D, Q       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9b422",
   "metadata": {},
   "source": [
    "## Small Encoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e68f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document encoding shape: torch.Size([1, 6, 64])\n",
      "Question encoding shape: torch.Size([1, 3, 64])\n",
      "\n",
      "Sentinel vector (document): tensor([-0.7055,  0.0055,  0.4913,  0.5456,  1.2552,  0.3021,  1.8662, -1.3343,\n",
      "        -1.3653,  1.3007,  0.5511, -1.1633, -0.5606,  0.6816,  1.4747,  0.0087,\n",
      "         0.0064,  1.1496,  1.4039,  0.2290,  0.0525, -1.1322, -0.1329,  0.6291,\n",
      "         0.5626, -0.0663,  0.1677, -0.6485, -0.0370, -0.2795,  3.0155,  0.1711,\n",
      "         0.6075, -1.8622, -0.2074, -1.4548,  0.5344, -0.2203, -0.3843,  0.5675,\n",
      "         0.4335, -0.9340,  0.9459, -2.3488,  1.3945,  0.4929,  0.2852,  0.7293,\n",
      "        -1.2620, -0.2164,  0.9424,  0.0091,  1.4794, -0.0702, -0.4078, -0.8863,\n",
      "         1.2518, -0.0994,  1.7704, -0.6406,  2.2154,  0.5434,  0.3357, -0.3190],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Sentinel vector (question): tensor([-0.0719,  0.7920,  0.1163,  0.0578,  0.2072, -0.8414, -0.0493, -0.3668,\n",
      "         0.2840, -0.2219, -0.0030,  0.0213, -0.4533,  0.4301,  0.7297,  0.2527,\n",
      "        -0.3950,  0.3298, -0.5550, -0.3452, -0.5325, -0.5140,  0.1792, -0.3787,\n",
      "        -0.2258, -0.4671, -0.4120, -0.6018, -0.4881,  0.1347,  0.8033, -0.2160,\n",
      "         0.1736,  0.1431, -0.1837, -0.7099, -0.1765,  0.4691, -0.1422,  0.4688,\n",
      "        -0.5401, -0.7789, -0.2399, -0.1653,  0.4438,  0.3714,  0.3294, -0.7875,\n",
      "         0.1591, -0.1699, -0.7671, -0.3849,  0.4657, -0.5298, -0.4053, -0.5794,\n",
      "        -0.0828, -0.2152,  0.3313, -0.3851,  0.0258,  0.2960, -0.2141,  0.4992],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Dummy vocab\n",
    "word2idx_test = {\n",
    "    \"[PAD]\": 0, \"[UNK]\": 1,\n",
    "    \"the\": 2, \"quick\": 3, \"brown\": 4, \"fox\": 5, \"jumps\": 6, \"over\": 7, \"lazy\": 8, \"dog\": 9\n",
    "}\n",
    "\n",
    "# Random embedding matrix for vocab (vocab_size x emb_dim)\n",
    "vocab_size = len(word2idx_test)\n",
    "embedding_dim = 50\n",
    "embedding_matrix_test = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Dummy inputs\n",
    "# Document: \"the quick brown fox jumps\"\n",
    "# Question: \"the fox\"\n",
    "doc_tokens = [2, 3, 4, 5, 6]\n",
    "q_tokens = [2, 5]\n",
    "\n",
    "# Padding to max length\n",
    "doc_max_len = 6\n",
    "q_max_len = 4\n",
    "doc_input = [doc_tokens + [0] * (doc_max_len - len(doc_tokens))]  # batch size 1\n",
    "q_input = [q_tokens + [0] * (q_max_len - len(q_tokens))]\n",
    "\n",
    "# Masks (1 for real tokens, 0 for padding)\n",
    "doc_mask = [[1]*len(doc_tokens) + [0]*(doc_max_len - len(doc_tokens))]\n",
    "q_mask = [[1]*len(q_tokens) + [0]*(q_max_len - len(q_tokens))]\n",
    "\n",
    "# Convert to tensors\n",
    "doc_idxs = torch.tensor(doc_input)      # [1, 6]\n",
    "doc_mask = torch.tensor(doc_mask)       # [1, 6]\n",
    "q_idxs = torch.tensor(q_input)          # [1, 4]\n",
    "q_mask = torch.tensor(q_mask)           # [1, 4]\n",
    "\n",
    "hidden_size = 64\n",
    "encoder = Encoder(hidden_size, embedding_matrix_test, 0)\n",
    "\n",
    "# Run encoder\n",
    "D, Q = encoder(doc_idxs, doc_mask, q_idxs, q_mask)\n",
    "\n",
    "# Outputs\n",
    "print(\"Document encoding shape:\", D.shape)  # [1, m+1, 64]\n",
    "print(\"Question encoding shape:\", Q.shape)      # [1, n+1, 64]\n",
    "\n",
    "print(\"\\nSentinel vector (document):\", D[0, -1])\n",
    "print(\"Sentinel vector (question):\", Q[0, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be400c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as functional\n",
    "\n",
    "\n",
    "class CoattentionEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, D, Q):\n",
    "        D_transposed = torch.transpose(D, 1, 2)\n",
    "        L = torch.bmm(Q, D_transposed)\n",
    "        AQ = functional.softmax(L, dim=1)\n",
    "        AQ_transposed = torch.transpose(AQ, 1, 2)\n",
    "        \n",
    "        AD = functional.softmax(L, dim=1)\n",
    "        CQ = torch.bmm(D_transposed, AQ_transposed)\n",
    "        \n",
    "        CD = torch.bmm(torch.cat([torch.transpose(Q, 1, 2), CQ], dim=1), AD)\n",
    "\n",
    "        brnn_input = torch.cat([torch.transpose(CD, 1, 2), D], dim=2)\n",
    "        return brnn_input\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#class CoattentionEncoder(nn.Module):\n",
    "#    def __init__(self):\n",
    " #       super().__init__()\n",
    "\n",
    "  #  def forward(self, D, Q):\n",
    "        # D: [batch, m+1, hidden]\n",
    "        # Q: [batch, n+1, hidden]\n",
    "\n",
    "        # Step 1: Affinity matrix\n",
    "   #     L = torch.bmm(Q, D.transpose(1, 2))  # [batch, n+1, m+1]\n",
    "\n",
    "        # Step 2: Attention weights\n",
    "    #    AQ = functional.softmax(L, dim=1)         # Q-to-D\n",
    " #       AD = functional.softmax(L.transpose(1, 2), dim=1)  # D-to-Q\n",
    "#\n",
    "        # Step 3: Context summaries\n",
    "  #      CQ = torch.bmm(AQ.transpose(1, 2), Q)      # [batch, m+1, hidden]\n",
    "   #     CD = torch.bmm(AD.transpose(1, 2), D)      # [batch, n+1, hidden]\n",
    "    #    CD_proj = torch.bmm(AQ.transpose(1, 2), CD)  # [batch, m+1, hidden]\n",
    "\n",
    "        # Step 4: Final representation\n",
    "     #   U = torch.cat([D, CQ, D * CQ, D * CD_proj], dim=2)  # [batch, m+1, 4 * hidden]\n",
    "     #   return U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "049091c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out= self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14dbd859",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m coattention_enc \u001b[38;5;241m=\u001b[39m coattention_enc\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[1;32m     11\u001b[0m U \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mbrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(probs)\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m, in \u001b[0;36mBRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 12\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m out\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1120\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1002\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_forward_args\u001b[39m(\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    999\u001b[0m     hidden: \u001b[38;5;28mtuple\u001b[39m[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     batch_sizes: Optional[Tensor],\n\u001b[1;32m   1001\u001b[0m ):\n\u001b[0;32m-> 1002\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1004\u001b[0m         hidden[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(\n\u001b[1;32m   1009\u001b[0m         hidden[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1012\u001b[0m     )\n",
      "File \u001b[0;32m~/dnlp2025/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:314\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 192"
     ]
    }
   ],
   "source": [
    "coattention_enc = CoattentionEncoder()\n",
    "brnn = BRNN(input_size=4 * hidden_size, hidden_size=hidden_size, num_layers=1, num_classes=2)\n",
    "U = coattention_enc(D, Q)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "brnn = brnn.to(device)\n",
    "coattention_enc = coattention_enc.to(device)  \n",
    "\n",
    "U = U.to(device)\n",
    "\n",
    "output = brnn(U)\n",
    "probs = torch.softmax(output, dim=1)\n",
    "print(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84639d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
