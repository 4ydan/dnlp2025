{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d51b8",
   "metadata": {},
   "source": [
    "# Download SQuAD Dataset and preprocess\n",
    "- Download Train + eval\n",
    "- tokenize data and write to separate files (context, question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7564883",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fac024b0",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37853986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import preprocess\n",
    "importlib.reload(preprocess)\n",
    "from preprocess import download_squad_dataset, process_split, write_to_files\n",
    "\n",
    "train, eval = download_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acdb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(train[\"data\"]))\n",
    "print(len(eval[\"data\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb712",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "- tokenization function (Stanford coreNLP tokenizer python only alternative)\n",
    "- mapping function: (context, context_tokens) -> dictionary mapping char indices to tokens: <br>\n",
    "example (\"this is a test\", [this, is, a, test]) ---> 0,1,2,3 -> (\"this\",0), 5,6 -> (\"is\",1), ... etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904dece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/workspace/dnlp2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-14 18:28:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 26.0MB/s]                    \n",
      "2025-06-14 18:28:05 INFO: Downloaded file to /home/luca/stanza_resources/resources.json\n",
      "2025-06-14 18:28:05 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-06-14 18:28:05 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-06-14 18:28:05 INFO: Using device: cpu\n",
      "2025-06-14 18:28:05 INFO: Loading: tokenize\n",
      "2025-06-14 18:28:07 INFO: Loading: mwt\n",
      "2025-06-14 18:28:07 INFO: Done loading processors!\n",
      "Processing articles: 100%|██████████| 48/48 [02:15<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mappingissues: 0\n",
      "spanissues: 0\n",
      "tokenissues: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors=\"tokenize\", tokenize_pretokenized = False)\n",
    "eval_dataset = process_split(eval, nlp)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f14537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = process_split(train)\n",
    "e_context_tokens, e_question_tokens, e_answer_tokens, e_span_tokens = write_to_files(eval_dataset, \"eval\")\n",
    "#t_context_tokens, t_question_tokens, t_answer_tokens, t_span_tokens = write_to_files(train_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47d61b",
   "metadata": {},
   "source": [
    "## Map tokens to embedding indices\n",
    "\n",
    "- load GloVe embeddings\n",
    "- map vocabulary to embedding indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048a1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/luca/workspace/dnlp2025/glove_embeddings/glove.840B.300d.txt\n",
      "Done!  2196016 words loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "glove_path = os.path.abspath(os.path.dirname(os.getcwd())) + \"/glove_embeddings/glove.840B.300d.txt\"\n",
    "print(glove_path) \n",
    "assert os.path.exists(glove_path), (\"glove embeddings file missing! Please download the correct embeddings and place them into the glove_embeddings directory\")\n",
    "embedding_index = {}\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        vals = line.split(' ')\n",
    "        word = vals[0]\n",
    "        coefs = np.asarray(vals[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "print(\"Done! \", len(embedding_index),\"words loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed61b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = []\n",
    "embedding_dim = 300\n",
    "embedding_matrix = []\n",
    "\n",
    "word2idx[\"[PAD]\"] = 0\n",
    "word2idx[\"[UNK]\"] = 1\n",
    "idx2word.append(\"[PAD]\")\n",
    "idx2word.append(\"[UNK]\")\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "embedding_matrix.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "\n",
    "def get_or_create_index(token):\n",
    "    token_lower = token.lower()\n",
    "    if(token_lower) in word2idx:\n",
    "        return word2idx[token_lower]\n",
    "    else:\n",
    "        idx = len(word2idx)\n",
    "        word2idx[token_lower] = idx\n",
    "        idx2word.append(token_lower)\n",
    "        if token_lower in embedding_index:\n",
    "            embedding_matrix.append(embedding_index[token_lower])\n",
    "        else:\n",
    "            embedding_matrix.append(np.random.normal(scale=0.01, size=embedding_dim))\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0302f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31338\n",
      "25764\n",
      "25764\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 12, 15, 8, 16, 17, 18, 19, 20, 12, 21, 22, 23, 12, 7, 8, 24, 17, 25, 19, 13, 26, 27, 28, 12, 15, 8, 24, 17, 29, 19, 13, 30, 31, 32, 33, 34, 10, 35, 36, 37, 2, 3, 38, 23, 12, 9, 5, 39, 40, 41, 42, 43, 44, 43, 45, 46, 47, 48, 12, 49, 50, 51, 52, 45, 53, 54, 43, 55, 23, 56, 57, 5, 12, 58, 2, 3, 43, 12, 16, 59, 12, 60, 61, 62, 60, 63, 64, 65, 66, 67, 68, 43, 56, 69, 56, 70, 71, 12, 72, 14, 73, 74, 2, 3, 9, 63, 75, 76, 17, 77, 78, 12, 9, 79, 80, 81, 82, 56, 60, 2, 3, 83, 60, 19, 43, 84, 85, 12, 86, 87, 88, 89, 12, 90, 76, 4, 23]\n"
     ]
    }
   ],
   "source": [
    "sen_idxs = []\n",
    "#do this for every token in contexts,question and answers\n",
    "all_tokens = []\n",
    "all_tokens.extend(e_context_tokens)\n",
    "all_tokens.extend(e_question_tokens)\n",
    "all_tokens.extend(e_answer_tokens)\n",
    "print(len(all_tokens))\n",
    "i = 0\n",
    "for tokens in all_tokens:\n",
    "    if tokens is None:\n",
    "        continue\n",
    "    idx = [get_or_create_index(t) for t in tokens.split()]\n",
    "    sen_idxs.append(idx)\n",
    "print(len(sen_idxs))\n",
    "print(len(word2idx))\n",
    "print(len(idx2word))\n",
    "print(sen_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9ee549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25764, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f147bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_matrix, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        vocab_size, embedding_dim = embedding_matrix.shape\n",
    "        embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True, dropout=dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "        self.w = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #self.b = nn.Parameter(torch.zeros(hidden_dim))\n",
    "\n",
    "        self.sentinel = nn.Parameter(torch.randn(1,hidden_dim))\n",
    "\n",
    "    def encode_sequence(self, idxs, mask):\n",
    "        lengths = mask.sum(dim=1)  # [batch]\n",
    "        sorted_lens, sorted_idx = lengths.sort(descending=True)\n",
    "        _, orig_idx = sorted_idx.sort()\n",
    "\n",
    "        # Sort sequences for packing\n",
    "        idxs_sorted = idxs[sorted_idx]\n",
    "        emb = self.embedding(idxs_sorted)\n",
    "        packed = pack_padded_sequence(emb, sorted_lens.cpu(), batch_first=True, enforce_sorted=True)\n",
    "\n",
    "        # LSTM encoding\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [batch, max_len, hidden]\n",
    "        out = self.dropout(out)\n",
    "        out = out[orig_idx]  # restore original order\n",
    "\n",
    "        # Insert sentinel at end-of-sequence index for each example\n",
    "        batch_size = out.size(0)\n",
    "        sentinel_expanded = self.sentinel.expand(batch_size, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "\n",
    "        out_with_sentinel = torch.cat([out, torch.zeros_like(sentinel_expanded)], dim=1)  # [batch, max_len+1, hidden]\n",
    "        lens = lengths.long().unsqueeze(1).unsqueeze(2).expand(-1, 1, self.hidden_dim)  # [batch, 1, hidden]\n",
    "        out_with_sentinel = out_with_sentinel.scatter(1, lens, sentinel_expanded)\n",
    "\n",
    "        return out_with_sentinel  # [batch, seq_len + 1, hidden]\n",
    "\n",
    "    def forward(self, doc_idxs, doc_mask, q_idxs, q_mask):\n",
    "\n",
    "        D = self.encode_sequence(doc_idxs, doc_mask)  # [batch, m+1, hidden]\n",
    "        Q_prime = self.encode_sequence(q_idxs, q_mask)  # [batch, n+1, hidden]\n",
    "\n",
    "        # Nonlinear projection: Q = tanh(W * Q′ + b)\n",
    "        Q = torch.tanh(self.w(Q_prime))  # [batch, n+1, hidden]\n",
    "\n",
    "        return D, Q       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9b422",
   "metadata": {},
   "source": [
    "## Small Encoder test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e68f2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document encoding shape: torch.Size([1, 6, 64])\n",
      "Question encoding shape: torch.Size([1, 3, 64])\n",
      "\n",
      "Sentinel vector (document): tensor([-0.4656, -0.0565, -0.5351, -0.9820, -0.0663,  1.0698, -1.5193, -0.6821,\n",
      "         0.0422,  0.7030,  0.7814, -0.9856,  0.0167, -0.8512,  0.1405, -1.1459,\n",
      "         0.6573,  0.2178,  0.4201, -0.6437, -1.3230, -1.3122, -1.6510, -1.2029,\n",
      "        -0.5488,  1.3250, -0.3112, -0.7705, -0.2678,  0.1188, -1.4871,  1.2600,\n",
      "         0.4149, -0.1634,  0.2989, -0.0226,  0.8531, -0.1789, -0.3517,  0.2735,\n",
      "        -0.8494,  0.8108, -0.2446, -0.1861,  1.5437, -0.4878,  0.9340, -0.7628,\n",
      "        -1.0553,  1.0785, -1.8988,  0.3790,  0.7689, -0.8682, -0.3757,  1.5796,\n",
      "         0.8995, -0.6768, -0.0731, -0.5980, -0.7956,  1.7212,  0.0497, -1.1951],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Sentinel vector (question): tensor([ 0.2639, -0.3731,  0.2077,  0.6458, -0.5799, -0.4256,  0.3102,  0.5333,\n",
      "        -0.5420,  0.1103, -0.4383,  0.4475, -0.4170,  0.4575,  0.2951,  0.1116,\n",
      "         0.0849,  0.1122, -0.0958,  0.0328,  0.0485,  0.2016, -0.4285, -0.1899,\n",
      "         0.2371,  0.1103, -0.5287, -0.3112, -0.5973, -0.4914,  0.7038,  0.3331,\n",
      "         0.6283, -0.6829,  0.2507,  0.8089, -0.3550, -0.0581, -0.5525,  0.2563,\n",
      "        -0.2288, -0.4524, -0.8088,  0.5119,  0.8080,  0.6302, -0.4064, -0.6464,\n",
      "         0.1363, -0.0798, -0.0662,  0.6083,  0.4620, -0.4056, -0.4491, -0.2917,\n",
      "        -0.4206,  0.3596,  0.2058, -0.4245, -0.1116,  0.3729, -0.3213, -0.3858],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Dummy vocab\n",
    "word2idx_test = {\n",
    "    \"[PAD]\": 0, \"[UNK]\": 1,\n",
    "    \"the\": 2, \"quick\": 3, \"brown\": 4, \"fox\": 5, \"jumps\": 6, \"over\": 7, \"lazy\": 8, \"dog\": 9\n",
    "}\n",
    "\n",
    "# Random embedding matrix for vocab (vocab_size x emb_dim)\n",
    "vocab_size = len(word2idx_test)\n",
    "embedding_dim = 50\n",
    "embedding_matrix_test = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Dummy inputs\n",
    "# Document: \"the quick brown fox jumps\"\n",
    "# Question: \"the fox\"\n",
    "doc_tokens = [2, 3, 4, 5, 6]\n",
    "q_tokens = [2, 5]\n",
    "\n",
    "# Padding to max length\n",
    "doc_max_len = 6\n",
    "q_max_len = 4\n",
    "doc_input = [doc_tokens + [0] * (doc_max_len - len(doc_tokens))]  # batch size 1\n",
    "q_input = [q_tokens + [0] * (q_max_len - len(q_tokens))]\n",
    "\n",
    "# Masks (1 for real tokens, 0 for padding)\n",
    "doc_mask = [[1]*len(doc_tokens) + [0]*(doc_max_len - len(doc_tokens))]\n",
    "q_mask = [[1]*len(q_tokens) + [0]*(q_max_len - len(q_tokens))]\n",
    "\n",
    "# Convert to tensors\n",
    "doc_idxs = torch.tensor(doc_input)      # [1, 6]\n",
    "doc_mask = torch.tensor(doc_mask)       # [1, 6]\n",
    "q_idxs = torch.tensor(q_input)          # [1, 4]\n",
    "q_mask = torch.tensor(q_mask)           # [1, 4]\n",
    "\n",
    "hidden_size = 64\n",
    "encoder = Encoder(hidden_size, embedding_matrix_test, 0)\n",
    "\n",
    "# Run encoder\n",
    "D, Q = encoder(doc_idxs, doc_mask, q_idxs, q_mask)\n",
    "\n",
    "# Outputs\n",
    "print(\"Document encoding shape:\", D.shape)  # [1, m+1, 64]\n",
    "print(\"Question encoding shape:\", Q.shape)      # [1, n+1, 64]\n",
    "\n",
    "print(\"\\nSentinel vector (document):\", D[0, -1])\n",
    "print(\"Sentinel vector (question):\", Q[0, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be400c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as functional\n",
    "\n",
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        return out\n",
    "\n",
    "class CoattentionEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.brnn = BRNN(input_size=3 * hidden_size,\n",
    "                         hidden_size=hidden_size,\n",
    "                         num_layers=num_layers)\n",
    "\n",
    "    #2.2 Coattention Encoder\n",
    "    def forward(self, D, Q):\n",
    "        #Affinity matrix\n",
    "        L = torch.bmm(Q, torch.transpose(D, 1, 2)) \n",
    "\n",
    "        #Attention weights\n",
    "        AQ = functional.softmax(L, dim=1)         \n",
    "        AD = functional.softmax(torch.transpose(L, 1, 2), dim=1)  \n",
    "\n",
    "        #Context Summaries\n",
    "        CQ = torch.bmm(AQ, D) \n",
    "        Q_combined = torch.cat([Q, CQ], dim=2)   \n",
    "        CD = torch.bmm(AD, Q_combined)\n",
    "\n",
    "        #BRNN\n",
    "        return self.brnn(torch.cat([D, CD], dim=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14dbd859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 128])\n",
      "1 6 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "coattention_enc = CoattentionEncoder(hidden_size).to(device)\n",
    "\n",
    "U = coattention_enc(D, Q)  \n",
    "print(U.shape)\n",
    "\n",
    "b,m,l = list(U.size())\n",
    "print(b,m,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c2110a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([8, 7]),\n",
      "indices=tensor([2, 3]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,8,4],[1,2,3,7]])\n",
    "y = torch.randn(4,4)\n",
    "x_u = x.max(1)\n",
    "print(x_u)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e2123",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size ,hidden_dim, maxout_pool_size, max_steps, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.max_steps = max_steps\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, 1, batch_first=True, dropout=dropout_ratio)\n",
    "\n",
    "        self.maxout_start = MaxOutHighWay(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "        self.maxout_end = MaxOutHighWay(hidden_dim, maxout_pool_size, dropout_ratio)\n",
    "\n",
    "    def forward(self, U, doc_pad_mask, target):\n",
    "        b,m,_ = list(U.size())\n",
    "\n",
    "        curr_change_mask_s, curr_change_mask_e = None, None\n",
    "\n",
    "        masks_s, masks_e, results_s, results_e, losses = [], [], [], [], []\n",
    "\n",
    "        # invert the document pad mask -> multiply padded values with smalles possible value -> no influence on loss computation\n",
    "        pad_mask = (1.0-doc_pad_mask.float()) * torch.finfo(torch.float32).min\n",
    "\n",
    "        idxs = torch.arange(0,b,out=torch.LongTensor(b))\n",
    "\n",
    "        #init start and end index to 0 and last word in document\n",
    "        s_idx_prev = torch.zeros(b,).long()\n",
    "        # sum evaluates to all words in document, since pad tokens == 0 and rest == 1 \n",
    "        e_idx_prev = torch.sum(doc_pad_mask,1) - 1\n",
    "\n",
    "        decoder_state = None\n",
    "        s_target = None\n",
    "        e_target = None\n",
    "        \n",
    "        #extract idx from given answer span\n",
    "        if target is not None:\n",
    "            s_target = target[:,0]\n",
    "            e_target = target[:,1]\n",
    "\n",
    "        #get previously computed start index coattention representation\n",
    "        u_s_idx_prev = U[idxs, s_idx_prev,:]\n",
    "\n",
    "        #decoder iterations (recommmended: 16)\n",
    "\n",
    "        for i in range(self.max_steps):\n",
    "            #get previously computed end index coattention represenation\n",
    "            u_e_idx_prev = U[idxs, e_idx_prev, :]\n",
    "            u_s_e = torch.cat((u_s_idx_prev, u_e_idx_prev), 1)\n",
    "\n",
    "            lstm_out, decoder_state = self.lstm(u_s_e.unsqueeze(1), decoder_state)\n",
    "            #extract final hidden state h_i\n",
    "            c_i, h_i = decoder_state\n",
    "\n",
    "            #compute new start index\n",
    "            s_idx_prev, curr_change_mask_s, loss_s = self.maxout_start(h_i, U, u_s_e, pad_mask, s_idx_prev, curr_change_mask_s, s_target) \n",
    "\n",
    "            #update start index with index computed above\n",
    "            u_s_idx_prev = U[idxs, s_idx_prev, :]\n",
    "            u_s_e = torch.cat((u_s_idx_prev, u_e_idx_prev), 1)\n",
    "\n",
    "            #compute new end index\n",
    "            e_idx_prev, curr_change_mask_e, loss_e = self.maxout_start(h_i, U, u_s_e, pad_mask, e_idx_prev, curr_change_mask_e, e_target) \n",
    "\n",
    "            if target is not None:\n",
    "                loss = loss_s + loss_e\n",
    "                losses.append(loss)\n",
    "\n",
    "            masks_s.append(curr_change_mask_s)\n",
    "            masks_e.append(curr_change_mask_e)\n",
    "            results_s.append(s_idx_prev)\n",
    "            results_e.append(e_idx_prev)\n",
    "\n",
    "        #retrieve last index predictions where updates halted\n",
    "        #idx should have shape (b,)\n",
    "        result_idx_s = torch.sum(torch.stack(masks_s,1),1).long() - 1\n",
    "        idx_s = torch.gather(torch.stack(results_s,1),1,result_idx_s.unsqueeze(1)).squeeze()\n",
    "        result_idx_e = torch.sum(torch.stack(masks_e,1),1).long() - 1\n",
    "        idx_e = torch.gather(torch.stack(results_e,1),1,result_idx_e.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        #compute loss while training and evaluating\n",
    "        if target is not None:\n",
    "            sum_losses = torch.sum(torch.stack(losses,1),1)\n",
    "            avg_loss = sum_losses/self.max_steps\n",
    "            loss = torch.mean(avg_loss)\n",
    "        print(f\"DEBUG: Before return - type(loss): {type(loss)}, value: {loss}\")\n",
    "        print(f\"DEBUG: Before return - type(idx_s): {type(idx_s)}, value: {idx_s}\")\n",
    "        print(f\"DEBUG: Before return - type(idx_e): {type(idx_e)}, value: {idx_e}\")\n",
    "        return loss, idx_s, idx_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dadcdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxOutHighWay(nn.Module):\n",
    "    def __init__(self, hidden_dim, maxout_pool_size, dropout_ratio=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.maxout_pool_size = maxout_pool_size\n",
    "        self.w_d = nn.Linear(5 * hidden_dim, hidden_dim, bias=False)\n",
    "        self.w_1 = nn.Linear(3 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        self.w_2 = nn.Linear(hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        self.w_3 = nn.Linear(2 * hidden_dim, hidden_dim*maxout_pool_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, h_i, U, u_s_e, pad_mask, idx_prev, change_mask, target=None):\n",
    "        b,m,_ = list(U.size())\n",
    "        #use view if dimensions dont match for cat\n",
    "        r_in = self.w_d(torch.cat((h_i.view(-1,self.hidden_dim), u_s_e),1))\n",
    "        r = functional.tanh(r_in)\n",
    "        print(\"r.shape after tanh: \",r.shape)\n",
    "        r = r.unsqueeze(1).expand(b,m,self.hidden_dim).contiguous()\n",
    "\n",
    "        m_t_1_in = torch.cat((U,r),2).view(-1, self.hidden_dim*3)\n",
    "        m_t_1, _ = self.w_1(m_t_1_in).view(-1, self.hidden_dim, self.maxout_pool_size).max(2)\n",
    "        print(\"m_t_1 shape: \", m_t_1.shape)\n",
    "\n",
    "        m_t_2, _ = self.w_2(m_t_1).view(-1, self.hidden_dim, self.maxout_pool_size).max(2)\n",
    "\n",
    "        score, _ = self.w_3(torch.cat((m_t_1,m_t_2),1)).max(1)\n",
    "        score = functional.softmax((score.view(-1,m) + pad_mask), dim=1)\n",
    "        _, idx = torch.max(score, dim=1)\n",
    "\n",
    "        if change_mask is None:\n",
    "            change_mask = (idx == idx)\n",
    "        else:\n",
    "            idx = idx * change_mask.long()\n",
    "            idx_prev = idx * change_mask.long()\n",
    "            change_mask = (idx!=idx_prev)\n",
    "\n",
    "        if target is not None:\n",
    "            loss = self.loss(score, target)\n",
    "            loss = loss * change_mask.float()\n",
    "        \n",
    "        return idx, change_mask, loss\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc384a",
   "metadata": {},
   "source": [
    "# Decoder Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff254857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Decoder with Dummy Inputs ---\n",
      "Dummy U shape: torch.Size([4, 50, 128])\n",
      "Dummy d_mask shape: torch.Size([4, 50])\n",
      "Dummy span shape: torch.Size([4, 2])\n",
      "DEBUG: Type of decoder object before call: <class '__main__.DynamicDecoder'>\n",
      "DEBUG: Is decoder None? False\n",
      "\n",
      "--- Running DynamicDecoder Forward Pass ---\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "r.shape after tanh:  torch.Size([4, 64])\n",
      "m_t_1 shape:  torch.Size([200, 64])\n",
      "DEBUG: Before return - type(loss): <class 'torch.Tensor'>, value: 2.6054880619049072\n",
      "DEBUG: Before return - type(idx_s): <class 'torch.Tensor'>, value: tensor([13, 12,  4, 15])\n",
      "DEBUG: Before return - type(idx_e): <class 'torch.Tensor'>, value: tensor([ 1, 12,  4, 48])\n",
      "\n",
      "--- An Error Occurred During Forward Pass ---\n",
      "cannot unpack non-iterable NoneType object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_26951/4138332594.py\", line 43, in <module>\n",
      "    loss, pred_s, pred_e = decoder(dummy_U, dummy_d_mask, dummy_span)\n",
      "TypeError: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "# --- Test Setup ---\n",
    "# Define small placeholder values for your dimensions\n",
    "BATCH_SIZE = 4\n",
    "DOCUMENT_LENGTH_M = 50 # Example max document length\n",
    "HIDDEN_DIM = 64     # Example hidden dimension (make it small for speed)\n",
    "MAX_DEC_STEPS = 3   # Example decoding steps (run at least 2-3 to see dynamic behavior)\n",
    "MAXOUT_POOL_SIZE = 4 # Example maxout pool size (p)\n",
    "\n",
    "print(\"--- Initializing Decoder with Dummy Inputs ---\")\n",
    "decoder = DynamicDecoder(\n",
    "    input_size=HIDDEN_DIM*4,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    maxout_pool_size=MAXOUT_POOL_SIZE,\n",
    "    max_steps=MAX_DEC_STEPS,\n",
    "    dropout_ratio=0.0 # Dropout isn't active in current code, but parameter is needed\n",
    ")\n",
    "\n",
    "# Create dummy inputs that mimic the expected shapes and types\n",
    "# U: (batch_size, document_length_m, 2 * hidden_dim) as u_t is 2l\n",
    "dummy_U = torch.randn(BATCH_SIZE, DOCUMENT_LENGTH_M, 2 * HIDDEN_DIM)\n",
    "print(f\"Dummy U shape: {dummy_U.shape}\")\n",
    "\n",
    "# d_mask: (batch_size, document_length_m) - True for valid, False for padding\n",
    "dummy_d_mask = torch.ones(BATCH_SIZE, DOCUMENT_LENGTH_M, dtype=torch.bool)\n",
    "# Simulate padding for one example\n",
    "if DOCUMENT_LENGTH_M > 10:\n",
    "    dummy_d_mask[0, 40:] = False # Pad 10 tokens for first sample\n",
    "    dummy_d_mask[1, 30:] = False # Pad 20 tokens for second sample\n",
    "print(f\"Dummy d_mask shape: {dummy_d_mask.shape}\")\n",
    "\n",
    "# span: (batch_size, 2) - true start and end indices\n",
    "# Make dummy span within valid range\n",
    "dummy_span = torch.randint(0, DOCUMENT_LENGTH_M - 1, (BATCH_SIZE, 2), dtype=torch.long)\n",
    "# Ensure start < end\n",
    "dummy_span[:, 1] = torch.max(dummy_span[:, 0] + 1, dummy_span[:, 1]) # ensure end > start\n",
    "print(f\"Dummy span shape: {dummy_span.shape}\")\n",
    "\n",
    "print(f\"DEBUG: Type of decoder object before call: {type(decoder)}\")\n",
    "print(f\"DEBUG: Is decoder None? {decoder is None}\")\n",
    "# --- Run the Forward Pass ---\n",
    "print(\"\\n--- Running DynamicDecoder Forward Pass ---\")\n",
    "try:\n",
    "    loss, pred_s, pred_e = decoder(dummy_U, dummy_d_mask, dummy_span)\n",
    "\n",
    "    print(\"\\n--- Forward Pass Completed Successfully! ---\")\n",
    "    print(f\"Final Loss: {loss.item()}\")\n",
    "    print(f\"Final Predicted Start Indices: {pred_s}\")\n",
    "    print(f\"Final Predicted End Indices: {pred_e}\")\n",
    "    print(f\"Final Predicted Start Indices Shape: {pred_s.shape}\")\n",
    "    print(f\"Final Predicted End Indices Shape: {pred_e.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An Error Occurred During Forward Pass ---\")\n",
    "    print(e)\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
