{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon1/dnlp2025/dnlp2025/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:glove:Loading GloVe embeddings from ../glove_embeddings/glove.840B.300d.txt\n",
      "INFO:glove:Processed 0 lines\n",
      "INFO:glove:Processed 100000 lines\n",
      "INFO:glove:Processed 200000 lines\n",
      "INFO:glove:Processed 300000 lines\n",
      "INFO:glove:Processed 400000 lines\n",
      "INFO:glove:Processed 500000 lines\n",
      "INFO:glove:Processed 600000 lines\n",
      "INFO:glove:Processed 700000 lines\n",
      "INFO:glove:Processed 800000 lines\n",
      "INFO:glove:Processed 900000 lines\n",
      "INFO:glove:Processed 1000000 lines\n",
      "INFO:glove:Processed 1100000 lines\n",
      "INFO:glove:Processed 1200000 lines\n",
      "INFO:glove:Processed 1300000 lines\n",
      "INFO:glove:Processed 1400000 lines\n",
      "INFO:glove:Processed 1500000 lines\n",
      "INFO:glove:Processed 1600000 lines\n",
      "INFO:glove:Processed 1700000 lines\n",
      "INFO:glove:Processed 1800000 lines\n",
      "INFO:glove:Processed 1900000 lines\n",
      "INFO:glove:Processed 2000000 lines\n",
      "INFO:glove:Processed 2100000 lines\n",
      "INFO:glove:Loaded 2196021 words with 300d embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from src.config import DCNConfig\n",
    "from src.squad import SquadDataset\n",
    "from src.model import CoattentionModel\n",
    "from src.glove import GloVeEmbeddings\n",
    "\n",
    "\n",
    "config = DCNConfig()\n",
    "\n",
    "glove = GloVeEmbeddings(embedding_dim=config.glove_dim)\n",
    "glove.load_glove_embeddings(config.glove_path)\n",
    "\n",
    "train_dataset = SquadDataset(glove.word_to_idx, split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=True)\n",
    "\n",
    "eval_dataset = SquadDataset(glove.word_to_idx, split=\"validation\")\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (2196021, 300)\n",
      "Vocabulary size: 2196021\n",
      "Index of 'the': 6\n",
      "Index of 'McDonald': 9172\n",
      "Index of '<PAD>': 0\n",
      "Index of '<UNK>': 0\n",
      "Total training samples: 87580\n",
      "Total validation samples: 10570\n",
      "\n",
      "\n",
      "Sample #33\n",
      "Context: The Panthers finished the regular season with a 15–1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49–15 in the NFC Cha...\n",
      "Question: What team did the Panthers defeat?\n",
      "Answer span: (25, 26)\n",
      "\n",
      "Embedded view:\n",
      "Context IDs: tensor([     6,  83217,   1697,      6,   1446,    547,     23,     10,    299,\n",
      "          7956,     70,    922,      4,      7,   9484,   7529,  74034,     34,\n",
      "          1654,      6,  22375,    126,   3505,    986,     18,  96888,     17,\n",
      "             5,     53,   8123,      6,  20240,  50141,   3847,   7956,    299,\n",
      "            11,      6, 147525,   6486,    243,      7,   2141,      8,     62,\n",
      "           345,   2261,   3975,   3071,    270,      6,   6426,     34,   4742,\n",
      "            11,   2858,      5,      6, 109082,   1697,      6,   1446,    547,\n",
      "            23,     10,    254,   7956,    135,    922,      4,      7,   6106,\n",
      "             6,     98,  17932,  41586,     10,   1012,      8,   6149,     62,\n",
      "          1112,     33,   2261,   3975, 492504,     28,  22746,     96,    266,\n",
      "          7956,    383,     11,      6, 157299,   6486,    243,      5,     53,\n",
      "          2485,      6,  41586,      4,  20402,  31732,      4,      7,  42514,\n",
      "         74567,     32,     55,      9,    554,   1700,     19,     35,    175,\n",
      "          2285,   9911,     11,      6,   2261,   3975,      5,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0])\n",
      "Context length: tensor([124])\n",
      "Question IDs: tensor([   82,   336,   131,     6, 83217,  6734,    41,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Question length: tensor([7])\n",
      "Answer span: tensor([25, 26])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding matrix shape: {glove.get_embedding_matrix().shape}\") # Should be (vocab_size, embedding_dim)\n",
    "print(f\"Vocabulary size: {len(glove.word_to_idx)}\")\n",
    "print(f\"Index of 'the': {glove.word_to_idx.get('the', 'Not found')}\")\n",
    "print(f\"Index of 'McDonald': {glove.word_to_idx.get('McDonald', 'Not found')}\")\n",
    "print(f\"Index of '<PAD>': {glove.word_to_idx.get('<PAD>', 'Not found')}\")\n",
    "print(f\"Index of '<UNK>': {glove.word_to_idx.get('<UNK>', 'Not found')}\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total validation samples: {len(eval_dataset)}\")\n",
    "print(\"\")\n",
    "\n",
    "sample_idx = 33\n",
    "print(f\"\\nSample #{sample_idx}\")\n",
    "print(f\"Context: {eval_dataset.context_data[sample_idx][:190]}...\")\n",
    "print(f\"Question: {eval_dataset.question_data[sample_idx]}\")\n",
    "print(f\"Answer span: {eval_dataset.answer_span_data[sample_idx]}\")\n",
    "# answer_text = eval_dataset.context_data[sample_idx]\n",
    "# print(f\"Answer: '{answer_text}'\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Embedded view:\")\n",
    "context_ids, context_len, question_ids, question_len, answer_span, qid = eval_dataset[sample_idx]\n",
    "\n",
    "print(f\"Context IDs: {context_ids}\")\n",
    "print(f\"Context length: {context_len}\")\n",
    "print(f\"Question IDs: {question_ids}\")\n",
    "print(f\"Question length: {question_len}\")\n",
    "print(f\"Answer span: {answer_span}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths_to_mask(lengths, max_len):\n",
    "    \"\"\"\n",
    "    Convert sequence lengths to binary masks.\n",
    "    \n",
    "    Args:\n",
    "        lengths: Tensor of shape (batch_size,) containing sequence lengths\n",
    "        max_len: Maximum sequence length. If None, uses the maximum from lengths\n",
    "        \n",
    "    Returns:\n",
    "        mask: Binary tensor of shape (batch_size, max_len) where 1 indicates valid positions\n",
    "    \"\"\"\n",
    "    # Create a range tensor [0, 1, 2, ..., max_len-1]\n",
    "    indices = torch.arange(0, max_len, dtype=lengths.dtype, device=lengths.device)\n",
    "    \n",
    "    # Expand dimensions: lengths -> (batch_size, 1), indices -> (1, max_len)\n",
    "    # Then broadcast and compare\n",
    "    mask = indices.unsqueeze(0) < lengths.unsqueeze(1)\n",
    "    \n",
    "    return mask.long()  # Convert boolean to 0/1 integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 6.2035, Validation Loss: 4.9628\n",
      "New best model saved!\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/it].94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 2.4814, Validation Loss: 4.9628\n",
      "New best model saved!\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.28s/it].23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 2.4814, Validation Loss: 4.9628\n",
      "New best model saved!\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.16s/it].54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 2.4814, Validation Loss: 2.4814\n",
      "New best model saved!\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/it].03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 3.7221, Validation Loss: 6.2035\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.99s/it].58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training Loss: 6.2034, Validation Loss: 4.9628\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it].58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training Loss: 2.4814, Validation Loss: 6.2035\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it].96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training Loss: 2.4814, Validation Loss: 6.2035\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.27s/it].63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training Loss: 2.4814, Validation Loss: 6.2035\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.88s/it].48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Training Loss: 2.4814, Validation Loss: 2.4814\n",
      "New best model saved!\n",
      "Saving model checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [01:16<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CoattentionModel(config.hidden_dim, config.maxout_pool_size, glove.get_embedding_matrix(), config.max_dec_steps, config.dropout_ratio)\n",
    "use_cuda = False\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.reg_lambda)\n",
    "\n",
    "# Training tracking\n",
    "best_eval_loss = float('inf')\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "os.makedirs(config.model_save_path, exist_ok=True)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Training started!\")\n",
    "for epoch in trange(10, desc=\"Epoch\"):\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        # Skip batches if enabled\n",
    "        if config.skip_frequency > 1 and iteration % config.skip_frequency != 0:\n",
    "            continue\n",
    "        \n",
    "        # Unpack the batch\n",
    "        context, context_lens, question, question_lens, answer_spans, _ = batch\n",
    "\n",
    "        context = context.to(device)\n",
    "        context_lens = context_lens.view(-1).to(device)\n",
    "        question = question.to(device)\n",
    "        question_lens = question_lens.view(-1).to(device)\n",
    "        answer_spans = answer_spans.to(device)\n",
    "\n",
    "        context_mask = lengths_to_mask(context_lens, config.context_len)\n",
    "        question_mask = lengths_to_mask(question_lens, config.question_len)\n",
    "        \n",
    "        # Reset gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # === Forward pass ===\n",
    "        loss, _, _ = model(context, context_mask, question, question_mask, context_lens, answer_spans)\n",
    "        \n",
    "        # === Backpropagation ===\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
    "        opt.step()\n",
    "\n",
    "        total_loss = loss.item()\n",
    "        epoch_train_loss += total_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        if num_batches % config.print_frequency == 0:\n",
    "            print(f\"Epoch: {epoch+1} Iteration: {iteration+1} loss: {total_loss}\")\n",
    "            \n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation at the end of each epoch\n",
    "    if (epoch + 1) % config.eval_frequency == 0:  # Note: moved to epoch level\n",
    "        print(\"Running validation...\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in eval_dataloader:\n",
    "                context, context_lens, question, question_lens, answer_spans, _ = val_batch\n",
    "                \n",
    "                # Move validation data to device\n",
    "                context = context.to(device)\n",
    "                context_lens = context_lens.view(-1).to(device)\n",
    "                question = question.to(device)\n",
    "                question_lens = question_lens.view(-1).to(device)\n",
    "                answer_spans = answer_spans.to(device)\n",
    "                \n",
    "                context_mask = lengths_to_mask(context_lens, config.context_len)\n",
    "                question_mask = lengths_to_mask(question_lens, config.question_len)\n",
    "\n",
    "                loss, _, _ = model(context, context_mask, question, question_mask, context_lens, answer_spans)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(eval_dataloader)\n",
    "        eval_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_save_path, 'best_model.pt'))\n",
    "            print(\"New best model saved!\")\n",
    "        \n",
    "        model.train()  # Switch back to training mode\n",
    "\n",
    "    # === Save model checkpoint ===\n",
    "    print(\"Saving model checkpoint...\")\n",
    "    torch.save(model.state_dict(), os.path.join(config.model_save_path, f'model_epoch_{epoch+1}.pt'))\n",
    "\n",
    "print(\"Training completed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Optional: Plot training curves\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      5\u001b[39m plt.plot(train_losses, label=\u001b[33m'\u001b[39m\u001b[33mTraining Loss\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Optional: Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig('training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon1/dnlp2025/dnlp2025/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\n",
      "Evaluating: 100%|██████████| 331/331 [13:12<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "model = CoattentionModel(config.hidden_dim, config.maxout_pool_size, glove.get_embedding_matrix(), config.max_dec_steps, config.dropout_ratio)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/best_model.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "predictions = OrderedDict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        context, context_lens, question, question_lens, answer_spans, qids = val_batch\n",
    "\n",
    "        context = context.to(device)\n",
    "        context_lens = context_lens.view(-1).to(device)\n",
    "        question = question.to(device)\n",
    "        question_lens = question_lens.view(-1).to(device)\n",
    "\n",
    "        context_mask = lengths_to_mask(context_lens, config.context_len)\n",
    "        question_mask = lengths_to_mask(question_lens, config.question_len)\n",
    "\n",
    "        _, start_indices, end_indices = model(context, context_mask, question, question_mask, context_lens, answer_spans)\n",
    "\n",
    "\n",
    "        batch_size = context.size(0)\n",
    "        for i in range(batch_size):\n",
    "            start = start_indices[i].item()\n",
    "            end = end_indices[i].item()\n",
    "            \n",
    "            context_tokens = [glove.index_to_word(idx.item()) for idx in context[i]]\n",
    "\n",
    "            # Clamp indices to valid range\n",
    "            start = max(0, min(start, len(context_tokens) - 1))\n",
    "            end = max(0, min(end, len(context_tokens) - 1))\n",
    "            if end < start:\n",
    "                end = start\n",
    "\n",
    "            predicted_tokens = context_tokens[start:end+1]\n",
    "            predicted_text = \" \".join(predicted_tokens).strip()\n",
    "\n",
    "            qid = qids[i]\n",
    "            if isinstance(qid, torch.Tensor):  \n",
    "                qid = qid.item() if qid.dim() == 0 else qid[0]\n",
    "\n",
    "            predictions[qid] = predicted_text\n",
    "\n",
    "# Save predictions to JSON\n",
    "with open(\"predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(\"Predictions saved to predictions.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(dataset, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    message = 'Unanswered question ' + qa['id'] + \\\n",
    "                              ' will receive score 0.'\n",
    "                    print(message, file=sys.stderr)\n",
    "                    continue\n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                prediction = predictions[qa['id']]\n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 1.22\n",
      "F1 Score: 4.13\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../dev-v1.1.json\", \"r\", encoding=\"utf-8\") as dev_file:\n",
    "    dev_data = json.load(dev_file)[\"data\"]  \n",
    "\n",
    "with open(\"predictions.json\", \"r\", encoding=\"utf-8\") as pred_file:\n",
    "    predictions = json.load(pred_file)  \n",
    "\n",
    "\n",
    "\n",
    "results = evaluate(dev_data, predictions)\n",
    "\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which NFL team represented the AFC at Super Bowl 50?\n",
      "Dev: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Which NFL team represented the NFC at Super Bowl 50?\n",
      "Dev: ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Where did Super Bowl 50 take place?\n",
      "Dev: ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Which NFL team won Super Bowl 50?\n",
      "Dev: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
      "Dev: ['gold', 'gold', 'gold']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What was the theme of Super Bowl 50?\n",
      "Dev: ['\"golden anniversary\"', 'gold-themed', '\"golden anniversary']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What day was the game played on?\n",
      "Dev: ['February 7, 2016', 'February 7', 'February 7, 2016']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What is the AFC short for?\n",
      "Dev: ['American Football Conference', 'American Football Conference', 'American Football Conference']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What was the theme of Super Bowl 50?\n",
      "Dev: ['\"golden anniversary\"', 'gold-themed', 'gold']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What does AFC stand for?\n",
      "Dev: ['American Football Conference', 'American Football Conference', 'American Football Conference']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What day was the Super Bowl played on?\n",
      "Dev: ['February 7, 2016', 'February 7', 'February 7, 2016']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Who won Super Bowl 50?\n",
      "Dev: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What venue did Super Bowl 50 take place in?\n",
      "Dev: [\"Levi's Stadium\", \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara\"]\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What city did Super Bowl 50 take place in?\n",
      "Dev: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: If Roman numerals were used, what would Super Bowl 50 have been called?\n",
      "Dev: ['Super Bowl L', 'L', 'Super Bowl L']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: Super Bowl 50 decided the NFL champion for what season?\n",
      "Dev: ['2015', 'the 2015 season', '2015']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What year did the Denver Broncos secure a Super Bowl title for the third time?\n",
      "Dev: ['2015', '2016', '2015']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What city did Super Bowl 50 take place in?\n",
      "Dev: ['Santa Clara', 'Santa Clara', 'Santa Clara']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What stadium did Super Bowl 50 take place in?\n",
      "Dev: [\"Levi's Stadium\", \"Levi's Stadium\", \"Levi's Stadium\"]\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n",
      "Q: What was the final score of Super Bowl 50? \n",
      "Dev: ['24–10', '24–10', '24–10']\n",
      "Pred: bowl\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N = 50\n",
    "count = 0\n",
    "\n",
    "for article in dev_data:\n",
    "    for paragraph in article[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            qid = qa[\"id\"]\n",
    "            question = qa[\"question\"]\n",
    "            dev_answers = [a[\"text\"] for a in qa[\"answers\"]]\n",
    "            predicted = predictions.get(qid, \"[NO PREDICTION]\")\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"Dev: {dev_answers}\")\n",
    "            print(f\"Pred: {predicted}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            count += 1\n",
    "            if count >= N:\n",
    "                break\n",
    "        if count >= N:\n",
    "            break\n",
    "    if count >= N:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
