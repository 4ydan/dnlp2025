{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.glove:Loading GloVe embeddings from glove_embeddings/glove.6B.300d.txt\n",
      "INFO:src.glove:Processed 0 lines\n",
      "INFO:src.glove:Processed 100000 lines\n",
      "INFO:src.glove:Processed 200000 lines\n",
      "INFO:src.glove:Processed 300000 lines\n",
      "INFO:src.glove:Loaded 400004 words with 300d embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from src.config import DCNConfig\n",
    "from src.squad import SquadDataset\n",
    "from src.model import CoattentionModel\n",
    "from src.glove import GloVeEmbeddings\n",
    "\n",
    "\n",
    "config = DCNConfig()\n",
    "\n",
    "glove = GloVeEmbeddings(embedding_dim=config.glove_dim)\n",
    "glove.load_glove_embeddings(config.glove_path)\n",
    "\n",
    "train_dataset = SquadDataset(glove.word_to_idx, split=\"train\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=True)\n",
    "\n",
    "eval_dataset = SquadDataset(glove.word_to_idx, split=\"validation\")\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=config.batch_size, num_workers=config.num_workers, shuffle=False)\n",
    "\n",
    "model = CoattentionModel(config.hidden_dim, config.maxout_pool_size, glove.get_embedding_matrix(), config.max_dec_steps, config.dropout_ratio)\n",
    "use_cuda = True if torch.cuda.is_available() else False\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (400004, 300)\n",
      "Vocabulary size: 400004\n",
      "Index of 'the': 4\n",
      "Index of 'McDonald': Not found\n",
      "Index of '<PAD>': 0\n",
      "Index of '<UNK>': 1\n",
      "Total training samples: 87580\n",
      "Total validation samples: 10570\n",
      "\n",
      "\n",
      "Sample #33\n",
      "Context: The Panthers finished the regular season with a 15–1 record, and quarterback Cam Newton was named the NFL Most Valuable Player (MVP). They defeated the Arizona Cardinals 49–15 in the NFC Cha...\n",
      "Question: What team did the Panthers defeat?\n",
      "Answer span: (25, 26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding matrix shape: {glove.get_embedding_matrix().shape}\") # Should be (vocab_size, embedding_dim)\n",
    "print(f\"Vocabulary size: {len(glove.word_to_idx)}\")\n",
    "print(f\"Index of 'the': {glove.word_to_idx.get('the', 'Not found')}\")\n",
    "print(f\"Index of 'McDonald': {glove.word_to_idx.get('McDonald', 'Not found')}\")\n",
    "print(f\"Index of '<PAD>': {glove.word_to_idx.get('<PAD>', 'Not found')}\")\n",
    "print(f\"Index of '<UNK>': {glove.word_to_idx.get('<UNK>', 'Not found')}\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total validation samples: {len(eval_dataset)}\")\n",
    "print(\"\")\n",
    "\n",
    "sample_idx = 33\n",
    "print(f\"\\nSample #{sample_idx}\")\n",
    "print(f\"Context: {eval_dataset.context_data[sample_idx][:190]}...\")\n",
    "print(f\"Question: {eval_dataset.question_data[sample_idx]}\")\n",
    "print(f\"Answer span: {eval_dataset.answer_span_data[sample_idx]}\")\n",
    "# answer_text = eval_dataset.context_data[sample_idx]\n",
    "# print(f\"Answer: '{answer_text}'\")\n",
    "print(\"\")\n",
    "\n",
    "# print(\"Embedded view:\")\n",
    "# context_ids, context_len, question_ids, question_len, answer_span = eval_dataset[sample_idx]\n",
    "\n",
    "# print(f\"Context IDs: {context_ids}\")\n",
    "# print(f\"Context length: {context_len}\")\n",
    "# print(f\"Question IDs: {question_ids}\")\n",
    "# print(f\"Question length: {question_len}\")\n",
    "# print(f\"Answer span: {answer_span}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Training tracking\n",
    "best_eval_loss = float('inf')\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "os.makedirs(config.model_save_path, exist_ok=True)\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Training started!\")\n",
    "for epoch in trange(config.num_epochs, desc=\"Epoch\"):\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for iteration, batch in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        # Skip batches if enabled\n",
    "        if config.skip_frequency > 1 and iteration % config.skip_frequency != 0:\n",
    "            continue\n",
    "        \n",
    "        # Unpack the batch\n",
    "        context, context_lens, question, question_lens, answer_spans = batch\n",
    "\n",
    "        context = context.to(device)\n",
    "        context_lens = context_lens.view(-1).to(device)\n",
    "        question = question.to(device)\n",
    "        question_lens = question_lens.view(-1).to(device)\n",
    "        answer_spans = answer_spans.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # === Forward pass ===\n",
    "        loss, _, _ = model(context, context_lens, question, question_lens, answer_spans)\n",
    "        \n",
    "        # === Backpropagation ===\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss = loss.item()\n",
    "        epoch_train_loss += total_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        if num_batches % config.print_frequency == 0:\n",
    "            print(f\"Epoch: {epoch+1} Iteration: {iteration+1} loss: {total_loss}\")\n",
    "            \n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation at the end of each epoch\n",
    "    if (epoch + 1) % config.eval_frequency == 0:  # Note: moved to epoch level\n",
    "        print(\"Running validation...\")\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in eval_dataloader:\n",
    "                context, context_lens, question, question_lens, answer_spans = val_batch\n",
    "                \n",
    "                # Move validation data to device\n",
    "                context = context.to(device)\n",
    "                context_lens = context_lens.view(-1).to(device)\n",
    "                question = question.to(device)\n",
    "                question_lens = question_lens.view(-1).to(device)\n",
    "                answer_spans = answer_spans.to(device)\n",
    "                \n",
    "                loss, _, _ = model(context, context_lens, question, question_lens, answer_spans)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(eval_dataloader)\n",
    "        eval_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_eval_loss:\n",
    "            best_eval_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(config.model_save_path, 'best_model.pt'))\n",
    "            print(\"New best model saved!\")\n",
    "        \n",
    "        model.train()  # Switch back to training mode\n",
    "\n",
    "    # === Save model checkpoint ===\n",
    "    print(\"Saving model checkpoint...\")\n",
    "    torch.save(model.state_dict(), os.path.join(config.model_save_path, f'model_epoch_{epoch+1}.pt'))\n",
    "\n",
    "print(\"Training completed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig('training_curves.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
